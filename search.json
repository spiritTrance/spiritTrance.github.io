[{"title":"【MIT6.172】学习笔记（2）","url":"/2024/02/09/MIT6.172_ch2/","content":"前言\n从这篇博客开始，就开始进入多线程程序的优化部分了。同样地，有错误请直接指出，十分感谢！\nLecture 6 多核编程\n多核出现的动机：频率无法进一步提高，大概在4GHz左右（功耗墙），摩尔定律。\n缓存一致性\n需要解决的问题：由于独立缓存的存在，多核对于相同物理地址所取得的数据要相同。因此有了MSI协议。这方面的详细介绍请见《计算机体系结构：量化研究方法》的第五章，此处不再详细介绍。\n并发平台\n首先把糟糕的代码放在这里，以便于提供良好示例。\n\npthreads\n这是ANSI/IEEE POSIX 1003.1-2008指定的标准API。线程通过共享内存进行通信。（进程间通信还有哪些方法？信号量，管道，套接字，消息队列，RPC...）\n\npthreads有两个很重要的函数pthread_create和pthread_join，这里给出编程模板，具体用法请查文档：\ntypedef struct&#123;    int64_t input;    int64_t output;&#125; thread_args;void *thread_func(void* ptr)&#123;    int64_t i = ((thread_args* ) ptr)-&gt;input;    ((thread_args *) ptr)-&gt;output = 2;    // ... do something    return NULL;&#125;pthread_t thread;thread_args args;args.input = 1;int status;pthread_create(&amp;thread, NULL, thread_func, (void*)&amp;args);status = pthread_join(thread, NULL);\n需要关注的问题是，线程创建的开销很大，大约在\\(10^4\\)cycles的数量级。可以考虑线程池的帮助。\nThreading Building Blocks(线程构建块，TBB)\n其由intel发明，作为本地线程之上的cpp库实现。程序员声明的是任务(tasks)而不是线程(threads)。使用work-stealing算法实现线程间的负载平衡。\n\n上图主要关注的是红色标出的函数。以及任务单位是task，需要用class提前定义并继承过来，需要声明构造函数和execute()方法。需要注意其中创建子任务的方法。\n需要注意set_ref_count的方法，其中参数是需要等待的任务个数（两个子任务，一个隐式声明的记账(bookkeeping)任务。spawb(b)表示启动任务b，spawn_and_wait_for_all(a)表示启动a并等待a和b任务的结束。\n最后的spawn_root_and_wait(a)的作用是启动根任务。\n需要知道的是，TBB提供了大量的cpp模板用于执行基本的操作，如：parallel_for用于循环并行化，parallel_reduce用于归约操作，pipeline和filter用于软件流水线。除此之外，还提供了concurrent container类，允许多线程并发安全地访问和更新容器内的内容。TBB还提供了mutual-exclusion库函数，包括锁和原子更新操作。\nopenmp\n行业协会的规范，几个编译器是提供的，如gcc，icc，clang和msvc。在本机线程上运行，提供循环并行，任务并行和管道并行。\n\n看得出来真的很简单，但是也很常用。后面另外开一个博客介绍openmp。\ncilk\n是对cpp的语法扩展。提供了监测竞争机制的工具cilkscreen和扩展性分析器cilkview。\nFibonacci示例：\n\n并行化循环示例：\n\n归约运算实例：\n\nLecture 7 竞争和并行\n确定性竞争\n确定性竞争：两个指令访问相同的物理内存位置，并且至少存在一个写操作，那么就是确定性竞争。竞争类型包括读竞争（一读一写）和写竞争（同时写）.如果使用的是cilk，可以使用cilksan工具检查是否有竞争出现。\n对于一个并行正序，可以建立computational DAG：\n\n\n阿姆达尔定律：如果一个应用有p(0&lt;p&lt;1)的部分必须串行执行，那么最大的加速比为1/p。\n如何根据计算图获取加速比呢？找计算图的关键路径即可。\n\n图中一个节点代表一个工作负载。\n令\\(T_p\\)表示程序在\\(p\\)个处理器上的工作时间，那么根据\\(T_1\\)和\\(T_p\\)的比值，有sublinear speedup（不知道怎么翻译好），线性加速比和超线性加速比的概念。加速比是两个时间之比。\n可扩展性分析\nTapir/LLVM编译器，提供可扩展性分析器Cilkscale。\n\n快速排序的时间复杂度为\\(O(nlgn)\\)，那么理论上的并行性为\\(O(lgn)\\)（无限个处理器后的时间复杂度为\\(O(n)\\)）（问题：在理论上如何分析？）\n下面是其他算法的并行时间复杂度。其中work是单核时间复杂度，span是无限多核心下的时间复杂度（怎么分析出来的？），并行性为work/span。\n\n调度理论\n本章主要探讨集中式调度器，不讨论分布式调度器。调度器负责将计算图的节点分配到处理器上。\n贪心调度算法：只要一个节点的前驱节点都处理好了，就开始分配处理。尽量在每一步都尽可能分配最多。\n\n完全步骤和不完全步骤无非就是描述处理器利用充不充分了。\n定理：基于贪心调度算法的耗时上限满足：\n\\(T_p\\leq T_1/P+T_{\\infin}\\)\n\n推论1：任何贪心调度器调度的结果所使用的时间最多在最优时间的二倍内实现。\n\n推论2：任何贪心调度器最多能实现近线性比加速，当算法满足\\(T_1/T_{\\infin}&gt;&gt;P\\)的时候。其中\\(T_1/(PT_{\\infin})\\)被称为并行松弛度(parallel slackness).\n\ncilk运行时系统\ncilk的work-stealing调度器能满足以下特征：\n\n每个处理器维护一个工作队列。这个工作队列表现得像双端队列。每个处理器每产生一次任务，就往自己的工作队列的底部压入相应的工作(spawn/call)，执行完，return的时候，就从自己的工作队列底部弹出相应工作。当一个处理器工作队列为空的时候，就会随机“窃取”其他不为空的工作队列的顶部的工作到自己的工作队列中。\n定理：充分的并行性使得“偷窃”很少发生，从而能够获得近线性加速比。换句话说，一个问题规模的可扩展性是看算法并行性写的好不好，如果写的不好，“偷窃”动作经常发生，带来开销，就难以达到线性加速比，从而导致问题不具备可扩展性。\n\n仙人掌堆栈数据结构的特点：维护一颗树，子节点视角下的栈，可以看到其祖先，一直到根节点。\n\n定理：规定串行算法所需的栈空间为\\(S_1\\)，P个处理器所需空间为\\(S_P\\)，则满足：\\(S_P\\leq PS_1\\).\nLecture 8 多线程程序算法分析\n分治递归\n主定理(Master Method)：\n&gt; 对于满足\\(T(n)=aT(n/b)+f(n)\\)的分治问题，满足\\(a\\geq 1, b\\geq 1\\)：满足：\n&gt; \n&gt; 证明略。推荐看ppt，有图，有意思。也可以看《算法导论》。\n&gt; 更一般的结论是Akra-Bazzi Method，有兴趣可以了解，更复杂。\ncilk循环(矩阵转置)，矩阵乘法，归并排序，打印数表\n这一段强烈建议看原ppt，我在这里相当于把ppt誊了一遍，所以略。主要讲了算法的实现，并行算法的时间复杂度和并行度的分析。都是举的例子看的。这里还是罗列一下需要掌握的内容：\n- cilk_for的内部实现也是类似于二分的写法，使用cilk_spawn和cilk_sync实现的，这样展开的时间就是\\(O(lgn)\\)。\n- 需要考虑分解问题的粒度，到了什么程度就开始不再分解，但分解程度需要小心考虑，最好建立理论模型再分析。\n- 分析一般分析两个时间复杂度：Work和Span，前者为单个处理器所花费时间，后者为无限个处理器，所花费的时间（也就是单个处理器所分配的任务所花时间），两者相除就是并行度。\n- 矩阵乘法的时间复杂度分析很有意思，还是截图下来看看：\n\n\n并且矩阵乘法考虑了主定理，有动态申请空间和不动态申请空间的问题。下面是时间复杂度分析\n\n\n在归并排序中，探究了序列划分的不平衡问题对时间复杂度带来的影响。属于算法分析的范畴了。\n\nLecture 10 测量和计时\n静默系统\n尽量减少系统的可变性，从而提高测量的精确度。下面这些是可变性的来源：\n\n在测量时，需要注意：\n- 没有其他任务运行\n- 关闭守护进程和定时任务\n- 不要连接网络\n- 不要摆弄鼠标！！！！！！！！！！！（引发中断！）\n- 串行任务不要使用0号cpu（因为0号cpu经常执行中断任务）\n- 关闭超线程\n- 关闭DVFS\n- 不要超频(Turbo boost)\n- 使用taskset将工作负责固定在某个核心中\n关于内存对齐：链接顺序的不同会导致机器码最终的不同，进而导致对齐状况不一样，可能会影响性能。LLVM编译器提供了一一些选项控制代码对齐：-align-all-functions=&lt;uint&gt;,-align-all-blocks=&lt;uint&gt;,-align-all-nofallthru-blocks=&lt;uint&gt;。值得一提的是，由于文件名最终也会出现在目标文件中，因此文件名也会影响数据对齐问题，进而影响性能。\n测量软件性能的工具\n有下列工具可以使用：\n- 外部测量：/usr/bin/times\n- real是墙钟时间，user和sys分别是用户所用时间和内核所用时间。\n- 程序源码修改：clock_gettime()\n- 注意rdtsc()有很多问题，不要用这个测量时间\n- 中断程序：gdb,pmprof,gprof等\n- 硬件和OS的支持：perf\n- 程序模拟：cachegrind\n- 比实时运行慢，但是可重复\n性能建模\n如果计算机有一些烦人的噪声，那么使用什么可以代表程序的性能呢？答案是最小值（而不是平均值或者其他）。\n下图是在不同场景下使用的不同性能度量：\n\n计算性能加速的平均值需要使用几何平均值（原因：使用算数平均值导致A/B和B/A是不可逆的）\n\nLecture 13 Cilk运行时系统\n略，有需要再学。\nLecture 20 推测并行\n\n这段代码中，abort_flag可能会出现争用(WAW)，但由于只可能从false变成true（不可能有其他结果），因此这里的争用是没有太大问题的。\n推测并行(Speculative parallelism)是指在并行程序中可能出现，但在串行程序中不会出现的行为。尽量不要出现这种行为，除非有在并行场合下有优化的机会。\n总结\n\n第6节课：多核编程\n\n了解缓存一致性的问题，MSI协议\n了解场景的多线程框架：\n\npthreads\nTBB\nopenmp\ncilk\n\n\n第7节课：竞争和并行\n\n确定性竞争的定义及种类\n扩展性问题\n并行程序的建模：计算图\n调度方案：贪心算法；work-stealing算法\n仙人掌堆栈数据结构\n\n第8节课：多线程程序算法分析\n\n主定理\n并行算法的时间复杂度，并行度分析（举了四个例子，更推荐找原ppt看）\n了解cilk_for的时间复杂度，并明白对并行算法所带来的时间复杂度分析的影响\n\n第10节课：测量和计时\n\n了解影响测量结果的场外因素\n了解测量常用的工具\n了解在不同场合下应该选择什么指标衡量程序性能\n了解测量加速比的平均值使用几何平均值及其原因\n\n第13节课：cilk运行时系统\n\n略，没学\n\n第20节课：推测并行\n\n了解推测并行的概念，存在竞争问题的代码什么时候才可以写出来。\n\n\n","categories":["学习笔记"],"tags":["MIT6.172"]},{"title":"【MIT6.172】学习笔记（1）","url":"/2024/02/09/MIT6.172_ch1/","content":"前言\n本文为MIT6.172的笔记，每篇文章最后对该篇文章的内容会做一简要总结。\n本系列假设读者对计算机组成原理，操作系统已经有清晰的认识。\n注意，这篇博客更倾向于作者的个人笔记，质量不太高，并且可能有事实上的错误（尽量避免）。如果发现，请随意批评指出。\nLecture 1 初步介绍&amp;矩阵乘法\n软件设计：有哪些数学更重要？兼容性，正确性，清晰，可debug，模块化，移植性，可维护性，功能性，可靠性，健壮性，可测试性，可用性。\n那么性能是什么？性能是用于“兑换”以上性质的“通货”。\n功耗墙：限制了cpu频率的进一步增长。后来转向多核设计。多核设计引入了更复杂的存储器体系结构（Shared memory, Distributed memory）\n摩尔定律：每过半年，晶体管的数目就要翻一番。（为什么近代开始失效？）\n以矩阵乘法为例：哪些因素会影响代码的执行？如何优化？\n①常规的矩阵乘法，速度为：Python&lt;Java&lt;C：\n\nPython为解释执行的语言\n\n解释器：灵活但速度缓慢，支持动态代码修改。工作原理为：读取一行代码，解释，执行，更新状态，如此循环\n\nJava先由编译为字节码（平台架构无关），再由解释器解释为对应平台的机器码\n\n即时编译(JIT compilation)：会将部分高频执行的字节码保存为机器码，加速执行，恢复因为解释而丢失的性能。\n\nC则是直接解释为对应的机器码\n\n② 缓存缺失：i，j，k的不同顺序会有影响，原因不再赘述。需要关注数据结构在内存中的排布。\n\n\n\n\n③编译优化\n针对C：-O0到-O3的优化程度越来越高（但实际性能则不一定），另外-Os是优化程序空间大小，-Og是为了debug。\n④并行化循环\n利用多核心，对循环进行并行化。记住，优先并行化外层循环，因为并行化操作本身也需要开销，并行化操作次数越少越好。（但不是所有代码都易于并行化）\n⑤块划分\n采用合适的划分方式，使得每块的访存次数尽量少：\n\n\n\n如图，每个核心负责算矩阵乘法某一小方块的内容即可。至于块大小如何划分，需要做实验才能知道。（每个核心负责算一块的内容）。在代码中，ih和jh相当于把一小块的base定下来了，k也相当于定了个base。\n⑥基于两级Cache的划分\n下图是现代处理器的常见架构。\n\n\n需要测试的参数包括s和t。\n⑦分治\nidea：矩阵乘法可以分解为更小的矩阵乘法（分块矩阵）\n\n需要强调的是这个矩阵是个方阵，如果需要支持其它矩阵，则需要进行修改。如果n不是2的幂，还需要仔细考虑边界问题（这个程序的assert就是用于判断是不是2的幂）。（作者的想法是，考虑满足r，c为2的幂的矩阵乘法的写法，然后把所有矩阵递归地分解为这样的矩阵。。。）\n其中：\n\ncilk_spawn表示并行化进行，cilk_sync表示同步，注意THRESHOLD判断条件。\n再注意mm_dac函数的入参：是原矩阵的大小，但n代表当前矩阵的大小。\n再注意那个宏展开：r是分块矩阵的行，c是分块矩阵的列，宏展开的结果是指针的偏移量，其实是子矩阵的base。详细地说，一个“单位”的大小为n/2，(r * (n_[A-C])) + c代表有多少个这样的单位。这样就好理解了。\n再回到mm_base函数，为什么要传入n_C,n_A,n_B，是为了方便计算偏移量。\nC[i][j] -&gt; C[i * n_C + j]这个该如何理解呢？这是需要联系内存排布的：|--|--||  |  ||--|--||  |√ ||--|--|考虑右下角的分块矩阵，我们有指针指向这块矩阵的第一个元素，那么该如何索引到这块矩阵的下一行元素的第一个元素呢？答案就是上式的转化。\n\n⑧编译器向量化优化\n需要开启-O2以上的优化。显示指定包括：\n\n-mavx：启用avx\n-mavx2：启用avx2\nmfma：启动乘法-加法向量指令\n-march=&lt;string&gt;：使用指定架构上的任何指令\n-march=native：使用本机架构的所有指令\n\n另外需要提到的是，使用某台架构的机器编译另外一种架构的编译行为叫做交叉编译。\n⑨手动向量化优化（内置向量化）\n使用内置函数。\n⑩baseline：Intel MKL(Math Kernel Library )\nLecture 2 Bentley工作量优化准则\n工作量：对于给定输入，一个程序所执行的所有操作，\n优化工作的角度：算法（很明显），利用计算机硬件的特性：ILP，cache，向量化，预测和分支预测，等等。\n\n数据结构\n\n打包和编码：将一些信息编码，使用更少的bit位进行表示。经典例子如一个int保存了32个flag，缺点是需要解码，增加编码困难。\n数据增广：使用额外的数据保存额外的信息，比如单向链表，可以保存尾指针，实现插入的常数时间插入。\n预计算：俗称打表\n编译期优化：还是打表\n缓存：将计算过的结果保存起来，以便于下一次用到。典型应用是记忆化搜索。\n稀疏性：某些数据结构，有大量无用数据，如稀疏矩阵，此时可以考虑用其它数据结构保存稀疏矩阵，以减少空间。\n\ntypedef struct&#123;    int n, nnz;    int * rows;    int * cols;    double* vals;&#125; spm;// 矩阵乘以向量。。。void spmv(spm *A, double* x, double* y)&#123;    for (in i = 0; i &lt; A-&gt;n; i++)&#123;        y[i] = 0;        for (int k = A-&gt;rows[i]; k &lt; A-&gt;rows[i+1]; k++)&#123;            int j = A-&gt;cols[k];            y[i] += A-&gt;vals[k] * x[j];        &#125;    &#125;&#125;\n\n代码逻辑\n\n常数折叠与传播：写过编译器的都知道这个方法，如果变量是常数，后面的表达式也是常数变量的话，那么编译期就可以算出这个结果。\n共同表达式消除：b = a - c, d = a - c =&gt; b - a - c, d = b\n代数恒等式：开方的开销是比平方大的，那么不妨把开方化成平方？\n短路运算：如果在循环中间已知结果，那么可以提早退出。\n条件排序：将最可能能得出整个表达式结果的条件排到前面\n创建快速路径：一个函数首先算出一些平凡常见的状况，避免开销更大的判断方法\n混合测试条件：其实就是用真值表，结合switch-case语句进行使用\n\n循环\n\n循环无关代码外提：不依赖于循环变量的语句全部扔到循环外面去\n哨兵值：如果循环内有判断语句，不妨考虑在数据结构里面嵌入哨兵值，减少判断语句的使用。\n\n循环展开：分为循环全展开和循环部分展开\n循环聚合：将循环范围相同的循环结合在一起\n消除多余的迭代：\n\n\n函数\n\n函数内联：减少函数调用开销\n尾递归：将递归语句放到return（最后）的位置，那么编译器会做优化，直接覆盖当前栈帧，而不是新开一个栈帧。\n粗粒度递归：当问题规模减小到一定程度时，如果继续递归下去，调用开销大于解决这个问题的开销时，就转为直接解决这个问题，而不是继续递归。\n\n其它建议\n\n避免过早的优化，先写正确的代码，保证正确性，使用回归测试\n编译器帮你干了以上很多工作，但是是否启用了，还是要查看编译器代码。\n\nTODO：介绍cpp的restrict和volatile关键字。。。\n\nLecture 3: bit hacks\n这一章就介绍了各种奇奇怪怪的bit operation，目的是：在编译器不自动优化时，自己手动优化（但绝大多数情况下编译器比你优化的好）。这里记录一下有意思的。\n\n无中间变量的交换\nx = x ^ y;y = x ^ y;x = x ^ y;\n但是不利于挖掘ILP\n寻找较小值\nr = y ^( (x ^ y) &amp; -(x &lt; y) );\n\n\n\n\nlowbit操作\nr = x &amp; (-x)\n八皇后问题\n记录状态可以用bit位操作，而不需要数组\n获取一个数的二进制的1的数量\nfor(r = 0; x!=0; r++)    x &amp;= x - 1\nstatic const int count[256] =&#123;0, 1, 1, 2, 1, ..., 8&#125;for (int r = 0; x != 0; x &gt;&gt;= 8)    r += count[x &amp; 0xff]\n\n\n还可以使用编译器内置函数（但移植性可能很差！）\nint __builtin_popcount (unsigned int x)\nLecture 4: 汇编语言和计算机体系结构\n编译的四个步骤：\n\n反汇编：objdump -S fib（前提是编译的时候带了-g选项）\nISA：定义了语法和语义。包括：寄存器，指令，数据类型，访存模式。\nx86-64指令集\nx86-64寄存器介绍\nx86-64有如下寄存器：\n\n其中通用寄存器，根据所使用的位宽不同，同一个寄存器有不同名字：一个寄存器根据使用的byte不同可能有多个名称，主要改动为寄存器前缀，但八位会改变后缀。[e**, r**, **, *h/*l]，\n\n部分寄存器可能采用其它名称，改变的是后缀。[**d, **w, **b]\n\n这些后缀的含义见下一节。\nx86-64指令介绍\nx86-64的指令，大致可以分为：数据移动，算数和位运算，以及跳转指令。其中指令后缀暗示数据类型。如subq表示64位整数。如果没有后缀，则会从操作数推断。\n\n下面是数据类型对应的后缀，可以稍微归纳一下，浮点数比较特殊：依次为s,d(ouble),t。其他的数据类型为b(yte), w(ord), l(ong)/d(ouble), q（毕竟本质都是整数）。特殊一点的是d，在浮点数和整数都出现过。\n\n下面是一些特殊指令的介绍：\n\n关于mov指令，有两种需要关注的后缀：标识扩展方式的后缀和标识数据位宽的后缀。如movzbl的z表示零扩展，bl表示从8bit移动到32bit；movslq的s表示符号扩展，lq表示从32bit移动到64bit。\n需要注意一个问题：32位扩展到64位采用的是隐式零扩展，和8位和16位的结果不同（是什么？待补）。\n关于j指令：通常按照RFLAGS保存d的内容来判断s是否跳转，而RFLAGS的值由cmp显式改变（其他指令的执行结果也会改变RFLAGS寄存器的值）。RFLAGS的内容如下：在正常情况下，只需要关注RFLAGS的四个域：CF，ZF，SF和OF。其中：\n\nCF表示上一个ALU运算是否进位\nZF表示上一个运算结果是否是0\nSF表示上一个运算结果的正负值，正0负1\nOF表示上一个运算结果是否溢出。\n\n对于j的判断条件后缀，如下表所示。如ja，jae等等。另外，思考一下ge的判断条件为什么是SF=OF？考虑不溢出的情况，那么ge意味着所算出的结果一定是正数，那么SF=OF=0；考虑溢出的情况（INT_MAX-INT_MIN），那么算出来的结果本身为正数，但是因为溢出变成了负数，那么SF=OF=1，也成立；（INT_MIN-INT_MAX）,那么结果为负数，下溢变成正数，那么SF=0, OF=1，也成立。\n最后注意j的操作数，可以是label，也可以是直接地址，也可以是间接地址：\njmp .L1jmp 5jmp *%eax\n### x86指令的寻址模式\n直接访存模式(Direct Addressing Mode)\n\n立即数：使用声明的立即数：movq $172, %rdi\n寄存器：使用指定存储器存储的数据：movq %rcx, %rdi\n内存访问：使用常数指定内存地址，使用对应内存地址所存储的数据，movq 0x172, %rdi\n\n间接访存模式(Indirect Addressing Mode)\n\n寄存器间接寻址(Register indirect)：使用寄存器所存的值作为内存地址：movq (%rax), %rdi\n寄存器基址寻址(Register indexed)：使用寄存器所存的值作为基地址，加上offset进行访存：movq 172(%rax), %rdi\n指令计数器相对寻址(Instruction-pointer relative)：使用%rip（类似于MIPS的PC，只不过x86是CISC\nCISC，指令是变长的，所以估计没有程序计数器这个概念）的值作为基地址。movq 172(%rip), %rdi\n基地址放缩位移寻址(Base Indexed Scale Displacement)\n\n需要注意的是地址的计算方式，以及两个寄存器是通用寄存器(GPR)。第一个GPR存base，第二个GPRc存Index。以及scale默认是1，可取1，2，4，8。\n\n\ntricks\n# 1xor %rax, %rax  # 用于清零寄存器# 2test %rcx, %rcx # test用于计算按位和，作用用于判断寄存器是否为0# 3data16 data16 data16 nopw %cs:0x0(%rax,%rax,1) # 空气泡，相当于nops\n浮点数与向量体系结构\n浮点数指令集\nx86支持float和double，x87加入了long double。SSE和AVX指令也包括了向量指令。编译器更喜欢使用SSE，因为SSE更容易编译和优化。对于SSE指令，使用两个字母的后缀来标识数据类型：[sp][sd]\n\n例：\nmovsd (%rcx,%rsi,8), %xmm1movsd %xmm1, (%rcx,%rsi,8)addsd (%rcx,%rsi,8), %xmm1mulsd %xmm0, %xmm1\n向量体系结构\n\n令k为向量位宽，每个向量寄存器保存k个整数或者浮点数。每个单元拥有k个向量车道，每个车道包含标量计算单元。每个车道使用相同的指令（也就是SIMD的体现了）。\n对于向量体系结构，有以下特点：\n\n内存排布可能需要对齐\n向量寄存器只能“按位操作”，即一个向量寄存器的第i个元素只能和另外一个向量寄存器的第i个元素进行操作\n某些体系结构支持车道交叉(cross-lane)操作，如插入或提取一个向量的子集，洗牌，分散和集中操作。\n\n下面是x86-64支持的标准：\n\nSSE：整数，单精度浮点数和双精度浮点数向量操作\nAVX：单精度浮点数和双精度浮点数向量操作\nAVX2：加入了整数向量操作\nAVX3(AVX-512)：扩大向量寄存器宽度到512，加入了新操作（如popcount，为1的个数）\n\nSSE和AVX*的比较：\n\nSSE使用xmm寄存器，长度为128bit，最多同时操作两个操作数\nAVX使用ymm寄存器，长度为256bit，最多接受三个操作数：vaddpd %ymm0, %ymm1, %ymm2，其中%ymm2为目标寄存器。\n\n最后，前缀也有两个进行表示：[v ][p ]，其中第一个前缀，带v则用avx，否则是sse；第二个前缀，带p则是整数，否则为浮点数。（别忘了前面的后缀）\n\n另外，向量寄存器也有类似于通用寄存器的别名：ymm的低128位为xmm，ymm总共有256位。\n计算机体系结构：概述\n首先介绍了经典五级流水线，这个应该都熟悉，下面看看Intel Haswell Microarchitecture的架构，大概有14-19级的流水线，先看看了解一下。\n\n从计算机体系结构上进行提升，主要有两种手段：\n\n开发ILP：如向量化，指令级并行，多核操作\n利用局部性原理：如使用cache\n\n超标量处理（实现多发射）\n使用流水线，但是需要关注三种数据依赖类型：真依赖(RAW)，反依赖(WAR)和输出依赖(WAW)。但由于部分操作时延较长，增加了流水线的设计复杂度，如下图所示。\n\n下图是Intel Haswell的功能单元设计：\n\n从流水线如何过渡到超标量？答案是进行多发射，保证每个功能单元都在执行任务（理想状况！）这里展示Intel Haswell的取指和解码阶段的架构：\n\n其采用了微指令体系结构，作用是将一条复杂指令分解为更多简单的指令。其包括一个复杂指令的解码器，每周期发射四条微指令。\n乱序执行\n流水线中一个重要的问题是数据依赖问题，对于RAW来说，常见的方案是数据前推(By passing)。对于一个指令流，我们可以建立一个指令依赖图。如下图所示\n\n而对于顺序发射，WAR和WAW似乎都不是问题，但如果是乱序执行呢？可以使用寄存器重命名技术！\n那么，乱序执行下，WAR和WAW可以通过重命名技术解决，而RAW造成的性能损失，在乱序发射的场景下，性能损失得到减少。下面将具体研究动态寄存器重命名技术。\n在体系结构中，会使用更多的寄存器保存因为WAR和WAW而多出来的值。维护这个关系的表叫做重命名表(renaming table)，其中维护ISA规定的寄存器号到物理寄存器号的映射。而物理寄存器由Freelist进行维护。\n关于寄存器重命名及动态排序的算法，推荐直接查找ppt看，图示较多，这里用我的语言介绍一下算法步骤：\n\n上图为重排序缓冲(reorder buffer, ROB)，用于跟踪指令间的依赖关系。每个指令拥有tag。source，dest以及重命名表的Data域可能取tag，也可能取物理寄存器。下面分两种情况讨论：\n\n当一个新的指令进入buffer时，首先检查source，根据重命名表判断是否来自未执行完的指令，并在ROB记录对应的tag，然后检查Dest，在重命名表中更新对应寄存器的tag。\n当一个指令执行完毕后，从ROB删除该项，并在重命名表中更新Data，如果tag和该指令的tag相同，则更新到对应的物理寄存器，并将ROB的对应Source项也进行更改（从tag改到物理寄存器）。\n\n总之，在乱序执行场景下，数据依赖关系可以通过图进行建模，而且只有RAW会影响性能，WAW和WAR几乎不影响性能。\n分支预测\n这一小节重点考虑控制冒险的问题。\n在IF阶段就需要知道分支判断的结果，但是需要在EXE阶段才能计算得到结果。因此有一些技巧：\n\n投机执行(Speculative Execution)：总是假定不执行，预测错误则撤销执行的部分带来的影响。缺点是流水线越深，分支预测错误带来的惩罚越严重。\n动态分支预测：分为局部动态分支预测和全局动态分支预测。局部是根据单条指令来判断，全局是根据所有指令的历史情况判断。\n\nLecture 5：从C语言到汇编\n为什么要了解汇编语言？因为汇编语言可以展示编译器做的优化，而且某些bug来源于编译器（开了O3独有的），而且可以手动修改汇编，使得其运行更加快。下图是基于LLVM的编译器的编译流程：\n\n生成LLVM的中间表示，然后通过优化器生成优化后的IR，最后生成汇编代码。首先看看IR的中间表示长什么样（使用clang编译器，使用-emit-llvm可以产生中间表示。\n\nLLVM中间表示\n\nLLVM IR和汇编的区别：更小的指令集，寄存器是无限的，没有FLAG计数器和条件码，没有显式栈帧指针\n\nLLVM IR寄存器：用%&lt;name&gt;表示，数量无限，相对于LLVM IR函数是局部的（即不同函数的寄存器编号可以相同）\nLLVM IR指令：\n\n产生结果的指令：%&lt;name&gt; = &lt;opcode&gt; &lt;operand list&gt;\n其他指令：&lt;opcode&gt; &lt;operand list&gt;\noperand可以是寄存器，常数和基本块\n指令列表如下：\n\nLLVM IR数据类型\n\n整数：i&lt;number&gt;。例：3bit整数：i3\n浮点数：float，double\n数组：[&lt;number&gt; x &lt;type&gt;]\n结构体：&#123;&lt;type&gt; ...&#125;\n向量：&lt; &lt;number&gt; x &lt;type&gt; &gt;\n指针：&lt;type&gt;*\n标签\n\n\n从C语言到LLVM IR\n基本块转IR\n数组结构常保存在内存中。而LLVM IR中的getelementptr函数用于从指针和下标计算内存地址。\n\n函数转IR\n\n从C语言到LLVM IR的函数，形参列表一一对应。并且按顺序自动命名为%0, %1, %2...\n\n基本块：一系列的指令，只有单个入口和单个出口。最后一条指令可以是跳转语句。\n控制流图：基于跳转情况，可以建立CFG，其中每个节点是一个基本块。\n条件语句转IR\n条件跳转语句(if)会转成br &lt;type&gt; &lt;reg&gt;, label &lt;reg&gt;, label &lt;reg&gt;，其中第一个参数用于判断，为真则跳转到第一个label，否则为第二个label。比较语句会转成icmp指令。条件跳转语句总会创建菱形CFG。\n\n\n其中pred记录了前驱基本块。\n无条件跳转语句则直接转换成br label &lt;reg&gt;，直接跳转。\n循环转IR\n循环包含循环体和循环控制两个部分。如下图所示。在CFG中会创建一个自循环部分。对于C语言的for循环，包含一个循环归纳变量(loop induction variable)，包含初始化，条件和自增的部分。\n\n注意归纳变量自增的时候，寄存器会替换，原因是循环自增重定义了变量（需要满足SSA，下文提到）\n\nLLVM IR维护静态单赋值（SSA, Static Single Assignment）的性质，即一个寄存器最多只能由一条指令赋值。这有利于进行数据流分析。但如果出现控制流的汇合，需要使用phi函数来维护这一性质：\n%9 = phi i64 [%14, %8], [ 0, %6 ]\n这个phi函数表示如果控制流从基本块8回来，则采用%14的值；从基本块6回来，则采用0。\nIR属性\nLLVM IR会为一些操作维护额外属性。这些属性的来源可能是源代码，也有可能是编译器分析。\n\n\n最后的总结：LLVM IR的特点是：所有计算出的值都会存入寄存器中，每个寄存器最多只有一条IR赋值，这个性质称为Static Single Assignment(SSA)，同时IR会被建模为CFG，进行进一步的优化。\n从LLVM IR到x86汇编\n这一步需要考虑三个问题：\n\n选择恰当的汇编指令\n合理分配通用寄存器\n处理函数调用\n\nx86-64的函数调用约定\n程序在虚拟空间中被划分为几个段，其中data段是初始化过的值，bss段是未初始化的值，默认初始化为0.\n\n汇编代码包括两种指令：引用部分(section)的指令和操作部分(section)的指令。其中：\n\n段指令将汇编代码文件组织为几个部分，包括.bss，.data，.text等。\n存储指令将相应的值存储在指定位置，如：\n\nx: .space 20：在x处划分20字节的空位\ny: .long 172：在y处存储172L\nz: .asciz \"6.172\"：在z处存储\"6.172\\0\"\n.align 8：下面的内容按照八字节对齐。\n\n域和链接指令：用于控制链接\n\n.globl fib使符号fib对其他目标文件可见。\n\n\n对于调用栈，有这些数据保存在栈上：返回地址，寄存器状态和函数参数，以及不能存入寄存器的局部参数。\nx86-64的函数调用约定：\n\n每个函数拥有自己的帧，其中%rbp保存栈帧顶，%rsp保存栈帧底。\n使用call和ret实现调用栈和退栈，指令指针寄存器%rip用于处理返回地址。其中call将%rip压栈，ret则进行弹栈操作。\n保存其他寄存器的内容：将寄存器分为caller和callee，前者需要调用者保存，后者由被调用者保存。其中callee-saved registers为%rbx, %rbp, %r12-%r15，其他的均为调用者保存寄存器。而%xmm0-%xmm7通常用来传递浮点数参数。\n\n原课程以Fibonacci为例再介绍了一下，略。愿意看的看原ppt即可。\nLecture 9 编译器能够做什么，不能做什么\n首先看一下编译器的图景，可以发现LLVM相当于编译器中端，向下屏蔽了高级语言类型，向上屏蔽了平台架构。\n\n如何优化？编译器通过多遍变换，生成优化后的LLVM IR。下面是Clang/LLVM生成优化报告的参数：\n\n-Rpass=&lt;string&gt;：生成关于指定优化方法的成功结果\n-Rpass-missed=&lt;string&gt;：失败结果\n-Rpass-analysis=&lt;string&gt;：前两者之和\n\n例：clang -O3 test.c -Rpass=.* -Rpass-analysis=.*\n编译器优化\n关于编译器能干的事罗列如下：\n\n### 标量优化\n\n上面这幅图需要注意的点：LLVM IR函数参数，将struct展开了（vec_t是两个double组成的结构体），第一个优化点是略去内存读取操作，直接在寄存器进行操作。注意%15和%20，实际上%14和%19都可以换成%2，并消除死代码，获得优化效果。\n结构体优化\n结构体很难处理，因为往往操作的是结构体的其中某一个域。\n\n观察上面这一块代码，最后%13需要什么数？%12实际是什么？%0，前面对内存进行了相当多的存入存出操作，最后发现就是%0，那么，可以进行替换，并进行死代码消除。\n\n总之，编译器尽量减少内存的读入读出操作，尽可能在寄存器内完成操作。\n函数调用优化\n①进行函数内联\n\n但需要注意的是，不是所有函数都可以进行内联，如递归调用（除了尾递归调用可以优化），不能将定义在其他编译后的单元的函数进行内联（除非一起编译），同时，函数内联会导致代码大小增大。\n编译器是否能判断函数是否内联呢？很遗憾的是并不知道，只能根据函数大小进行猜测。可以利用__attribute__((always_inline))对函数进行标记，令相应函数进行内联。使用__attribute__((no_inline))取消内联。同时，可以使用链接时优化(link-time optimization (LTO) )进行整体编译的优化。\n循环优化\n无关代码外提(code hoisting)，或无关代码移动(loop-invariant code motion, LICM)，目的在于将不依赖于循环归纳变量(loop induction variable)的语句提到循环外。\n\n如图，里面两个框起来的语句只依赖于外层变量，因此可以考虑外提这些语句。对应的源代码如下。\n\n但是编译器无法发掘什么问题呢？上个图的函数作用是计算力的大小，编译器是无法发现两个物体的相互作用力，大小相同，方向相反这个特点的。（背景是N体运动模拟问题）\n失败的优化——样例学习\n样例1 ：向量的线性运算\n\n观察这段代码，这个分支是什么呢？其实是判断两个数组是不是一个东西，然后根据结果跑不同的指令。可以看到，编译器产生了两个版本的指令，因为存在内存别名(memory aliasing)的问题！可以使用restrict关键字修饰入参，表明这两个地址一定不是同一个地址，从而消除内存别名的问题。同时，const修饰符表明只读的属性。\n样例2：归一化\n\n将每次计算都能得到相同结果的，但计算代价大的成分扔出循环。但编译器为什么没有优化？因为编译器不知道传入的参数，内容是否会被修改！因此，在norm函数声明的最前面可以加上__attribute__((const))来声明不会有修改，从而达到优化目的。\n样例3：显式循环展开\n\n如图，由于循环次数不确定，无法进行向量化。为什么？因为传入参数是size_t，这是个无符号整数！注意由于无符号整数的溢出是个Undefined Behavior，编译器的行为不确定（可能优化可能没有优化）。修改方法是改用有符号整数。\n链接时优化\n主要是单个编译单元优化程度不够，可以进行链接时优化，clang编译器开启-flto将源代码编译成LLVM IR形式，带上-fuse-ld=gold将LLVM IR代码链接在一起。\n总结\n本篇文章对第1节课到第5节课，以及第9节课的内容进行了总结，其中：\n\n第1节课\n\n以矩阵乘法为例，介绍了一系列优化的效果\n\n第2节课\n\n介绍了一些优化技巧，并且这些技巧大多数可以被编译器识别，但仍然需要了解\n\n第3节课\n\n关于位操作的一些骚操作\n\n第4节课\n\n介绍了x86 ISA，其中需要掌握的内容有：\n\n所拥有的寄存器，作用，以及寄存\n器别名的设计，寄存器的前缀，后缀的含义\n常见指令，以及常见指令的后缀\nx86的条件RFLAGS寄存器的相关设计(ZF,OF...)\nx86的寻址模式\n向量指令集 SSE，AVX，AVX2和AVX3的特点\n指令集的前缀([v ][p ])后缀([sp][sd])\n\n之后介绍了计算机体系结构相关的内容，主要是从五级流水线到现代处理器流水线特征的过渡，包括：\n\n向量体系结构\n超标量处理\n乱序执行：数据冒险的处理，以及针对WAR和WAW的寄存器重命名技术\n分支预测：投机执行和动态分支预测\n\n\n第5节课\n\n对LLVM IR进行了初步介绍，需要知道\n\n基本块的定义\nSSA，phi函数\nCFG\n\n进程的虚拟地址空间组织方式，有哪些段以及作用（.text,.bss,.data...）\n汇编代码文件的组织，汇编代码的一些特殊指令\nx86-64的函数调用约定\n\n第9节课\n\n了解编译器的优化问题，包括\n\n编译器输出优化报告\n通过减少访存次数，尽量在寄存器中进行操作，同时消除死代码\n函数内联，哪些函数不能函数内联，以及优缺点\n影响编译器优化的因素，包括：\n\n寄存器别名问题，C语言的关键字restrict\n不确定的内存修改：对相关函数加上__attribute__((const))修饰符，表明相应函数不会对传入参数的内容进行修改。\nundefined behavior行为导致优化的不确定性：如无符号整数导致的溢出。\n\n链接时优化\n\n\n\n","categories":["学习笔记"],"tags":["MIT6.172"]},{"title":"【cpp查漏补缺】2-Makefile入门","url":"/2023/10/01/cpp_2_Makefile%E7%94%A8%E6%B3%95/","content":"前言\n上一节中我们简单介绍了cpp是如何一步步构建出可执行文件的。但在大型工程中，我们常用Make来构建项目。本文简单介绍Makefile的编写规则。\n入门级Makefile介绍\n宝宝级的项目结构\n项目结构如下：\nspirittrance@hexadecimal:~/Desktop/learnMakefile$ tree.├── bin├── build├── include│   └── calc.h├── lib├── Makefile└── src    ├── add.cpp    ├── div.cpp    ├── main.cpp    ├── mul.cpp    └── sub.cpp\n其中main.cpp内容如下\n#include &lt;iostream&gt;#include &quot;calc.h&quot;using namespace std;int main()&#123;    cout &lt;&lt; &quot;Add: &quot; &lt;&lt; myadd(9, 3) &lt;&lt; endl;    cout &lt;&lt; &quot;Sub: &quot; &lt;&lt; mysub(9, 3) &lt;&lt; endl;    cout &lt;&lt; &quot;Mul: &quot; &lt;&lt; mymul(9, 3) &lt;&lt; endl;    cout &lt;&lt; &quot;Div: &quot; &lt;&lt; mydiv(9, 3) &lt;&lt; endl;    return 0;&#125;\ncalc.h内容如下：\n#ifndef __CALC_H__#define __CALC_H__    int myadd(int a, int b);    int mysub(int a, int b);    int mymul(int a, int b);    int mydiv(int a, int b);#endif\n除此之外，如果使用的是Vscode，请记得使用相关扩展，进行相应配置。如C++ Extension，以便于进行快速查找。\n最简单的Makefile\n在本节开始，我首先给出最简单的示例：\n# Easy Makefilemain: src/main.cpp src/add.cpp src/sub.cpp src/mul.cpp src/div.cpp\tg++ -I ./include src/adda.cpp src/sub.cpp src/mul.cpp src/div.cpp src/main.cpp -o bin/mainclean:\trm -rf bin/*\n\n注意cpp文件需要使用g++，如果是gcc的话会报错（因为使用了iostream，gcc貌似找不到）\n\n关于Makefile的编写规则，有三个要素：目标，依赖，执行语句（动作）。\n如上例，main即为目标，后面紧跟的一长串为依赖，下面那个是执行语句。需要在这里强调的是，执行语句前面的符号是制表符\\t而不是四个空格，这一点务必注意，否则你会看到missing separator的错误。\n在终端输入make main后，可以发现bin下面多了可执行文件。另外如果只使用make，那么默认执行第一个语句。如果输入make clean，那么会执行clean后的语句rm -rf bin/*，会清除bin目录下的二进制文件。\n话说回来，所谓的依赖有什么用呢？你可以尝试把add.cpp重命名为add_.cpp，然后在构建语句中修改add.cpp为add_.cpp，但不要修改依赖。之后执行make，你会发现有如下报错：\nmake: *** 没有规则可制作目标“src/add.cpp”，由“main” 需求。 停止。\n然后将依赖删除，再次执行make，你会发现能够顺利编译出可执行文件。因此，依赖的作用是检查源文件是否存在。如果不存在，则会阻止执行相应语句。\nwildcard函数\n在上一小节中，我们用\"紧跟的一长串\"来描述那后面的一大堆源文件，这确实很长。如果项目再大一点，我们也不可能把所有文件一一罗列出来。那么，我们是否有更好的编写方法呢？答案是有，那就是使用wildcard函数。下面给出修改后的示例：\nSOURCE=$(wildcard src/*.cpp)main: $(SOURCE)\tg++ -I ./include $(SOURCE) -o bin/mainclean:\trm -rf bin/*\n其中，我们用SOURCE变量来标识源文件，之后按照$(SOURCE)的方式来引用变量，之后是wildcard函数的用法：\n$(wildcard pattern)\nwildcard函数的作用在于匹配所有模式，如*.cpp会匹配所有以.cpp结尾的文件。\n最后，我们介绍一下其他的赋值方法：\na=suba+=set      # 增添赋值，a的结果为subsetecho $(a)   # subsetb?=superseta?=superset # 条件赋值，如果该变量未被赋值，那么使用后面给定的值echo $(a)   # subsetecho $(b)   # superset\n进阶Makefile\n更复杂的项目结构\n实际项目的项目结构只会更复杂，比如，源文件下还有多层文件夹，每个文件夹下面又有很多文件夹和源文件，这种情况，应该怎么办呢？\n这里我们修改一下项目结构：\n.├── bin                         # 存放二进制可执行文件│   └── main├── include                     # 存放头文件│   └── calc.h├── lib                         # 存放相应的库│   └── libcalc.a├── Makefile                    # Makefile├── output                      # 中间代码文件│   └── src│       └── calc│           ├── add_sub│           │   ├── add.o│           │   └── sub.o│           └── mul_div│               ├── div.o│               └── mul.o└── src                         # 源文件    ├── calc    │   ├── add_sub    │   │   ├── add.cpp    │   │   └── sub.cpp    │   └── mul_div    │       ├── div.cpp    │       └── mul.cpp    └── main.cpp\n没错，我们把add.cpp和sub.cpp移动到了src/calc/add_sub下面，而mul.cpp和div.cpp移动到了src/calc/mul_div下面。\nforeach函数\n现在应该怎么做呢？我们可以选用foreach函数。现在仍然放出修改后的Makefile.\nSOURCE_DIR=src src/calc/add_sub src/calc/mul_div    # 定义了包含源代码的所有文件夹SOURCE=$(foreach dir,$(SOURCE_DIR),$(wildcard $(dir)/*.cpp))    # foreach函数的应用INCLUDE=./include                                   # include的文件夹TARGET_DIR=bin                                      # 目标文件所在文件夹TARGET=main                                         # 目标文件名FLAGS=-Og -I $(INCLUDE)                             # 所有标记FLAGS+=$(OPTFLAGS)                                  # 可扩展性要求$(TARGET): $(SOURCE)                                # 这里进行了相应的修改，请注意\tg++ $(FLAGS) $(SOURCE) -o $(TARGET_DIR)/$(TARGET)clean:\trm -rf bin/*test:    echo $(SOURCE)\n这里我们先介绍一下foreach函数的用法：\n\nforeach(var,list,text)\n该函数的用法是：从list逐个取出值赋值给var，之后使用text进行相应的展开。注意text可以引用变量var。如果你想知道上例的结果，可以使用make test查看结果。\n注意到Makefile多了一个OPTFLAGS的变量，但该变量未定义，该怎么用呢？你可以在命令行输入make OPTFLAGS=-Wall（用途是打开警告信息），然后观察编译结果有何不同。\n如果你不加以定义，即直接输入make，那么默认是空，没有影响。\n\n\n如果对源代码的结构不熟悉，但是希望把所有的cpp文件包含进来（也就是不知道层级结构是啥情况），又应该怎么办呢？待补充。（也许会用到函数）\n\npatsubst函数\n在1-cpp是如何跑起来的中，我们介绍了cpp的构建过程，一般来说，我们希望拿到目标文件并加以保存，减少编译时间。现在，我们希望能够生成中间代码并进行编译，应该如何做呢？你会想到g++的-c选项，但如果是Makefile，如何进行相应的编写呢？这就需要patsubst函数了。下面给出修改后的Makefile：\n# Advanced Makefile# - source variablesSOURCE_DIR=src                                  # 源代码目录SOURCE_MAIN=$(SOURCE_DIR)/main.cpp              # 源代码main函数路径SOURCE_CALC=$(foreach source,$(wildcard src/calc/*), $(wildcard $(source)/*.cpp))   # 源代码计算函数相关路径# - obj file variablesOBJECT_DIR=output                                                   # 目标代码文件目录OBJECT_CALC=$(patsubst %.cpp,./$(OBJECT_DIR)/%.o,$(SOURCE_CALC))    # 与计算函数相关的目标代码文件路径# - library file variablesLIB_NAME=calc                                                       # 库名称LIB_DIR=lib                                                         # 库所在的目录LIB_CALC=$(LIB_DIR)/lib$(LIB_NAME).a                                # 库所在的路径# - target binary file variablesTARGET=bin/main                                                     # 可执行文件的路径# - include variablesINCS=./include                                                      # 包含文件所在目录# generate target binary executable file$(TARGET): $(SOURCE_MAIN) $(LIB_CALC)                               \tg++ -I $(INCS) -L ./lib $(SOURCE_MAIN) -l$(LIB_NAME) -o $(TARGET) # generate static library$(LIB_CALC): $(OBJECT_CALC)\tar -rcs $(LIB_CALC) $(OBJECT_CALC)# generate object file./$(OBJECT_DIR)/%.o: %.cpp\t@mkdir -p $(dir $@)\tg++ -c $&lt; -o $@# cleanclean:\trm -rf bin/*testPatsubst:    @echo $(OBJECT_CALC)\n上面所给出的示例有相当大的变化，我们首先解释patsubst函数。\npatsubst %.cpp,str%.o,$(var)\n这一函数接受一个var，其中var应当是一个列表（linux的字符串列表），在列表中寻找后缀为.cpp的元素，并保留.cpp前的字符串，并在前面拼接上str的字符串，将后缀改为.o，然后输出一系列列表。本例中，你可以尝试make testPatsubst观察输出。\n\n结果为./output/src/calc/add_sub/add.o ./output/src/calc/add_sub/sub.o ./output/src/calc/mul_div/div.o ./output/src/calc/mul_div/mul.o\n\n依赖问题\n在开始后面几节之前，我先介绍一下这个Makefile做了什么事：首先，将add.cpp,sub.cpp,mul.cpp和div.cpp分别生成相应的目标文件，之后将四个目标文件进行归档，生成静态库libcalc.a，最后编译main.cpp并与该静态库链接，生成可执行文件main。可以看到，所有行前后都有依赖关系：$(TARGET)依赖$(LIB_CALC)，$(LIB_CALC)依赖$(OBJECT_CALC)，而$(OBJECT_CALC)依赖$(SOURCE_CALC)（在Makefile中用./$(OBJECT_DIR)/%.o: %.cpp表示，这一行语句的含义将马上介绍），构成了级联的依赖关系。\n\n在开始下一节内容之前，我希望你能在编译完整的基础上，依次做如下事情：\n1.删除lib/libcalc.a，但保留output/*，之后make\n2.不删除lib/libcalc.a，但删除output/*，之后make\n3.删除lib/libcalc.a，同时删除output下的一个或多个文件，之后make\n4.删除lib/*和output/*，之后make\n5.修改任意源文件，然后make\n6.修改头文件，然后make\n做完这三件事后，你会对cpp的编译过程有更深的理解，以及能够体会到为什么Makefile要维护所谓的依赖关系。比较特殊的是第6条，这一条我们在【自动生成依赖】一节讨论。\n\n模式匹配与变量\n在上一节中，我们注意到如下语句：\n./$(OBJECT_DIR)/%.o: %.cpp  # OBJECT_DIR=output\t@mkdir -p $(dir $@)\tg++ -c $&lt; -o $@\n首先介绍一下，目标和依赖的%是什么意思。回忆patsubst函数，不难猜出这一句话的含义是：\n\n如果要生成形如./output/src/str.o的目标文件，需要依赖形如./src/str.cpp的源文件，其中str是可以任意更换的，因此用%表示。这一种形式实际上是在做模式匹配。\n\n之后我们把注意力留在$@和$&lt;这两个变量上面。根据第二条执行语句，不难猜出，$@代指目标，$&lt;代指依赖。准确来说，是第一个依赖。\n最后，我们把注意力留在第一条执行语句。mkdir是一条linux的指令，前面加上@，意思是不要打印执行的语句。而dir函数和linux的dir函数含义不同。在Makefile里面，指的是返回相应文件所在的目录。如$@为./output/src/str.o的情况下，dir $@返回./output/src。\n到这里，这一部分的关键之处便解释完毕。最后需要指出的是，使用./$(OBJECT_DIR)/%.o而不是%.o的原因在于，希望不要将生成文件和源文件混合存放。因此，在本小节的项目结构里面，你可以看见output下面有src目录。\n伪目标\n在最后，我们调用clean的时候，我们通常都能成功执行。但如果该目标缺少依赖，并且目录下同样有一个名为clean的文件，进行make clean的时候，你会得到：\n\nmake: “clean”已是最新。\n\n这个时候，你需要在前面加上：\n.PHONY:clean\n表明clean并不是一个真正的目标，只是一个伪目标。之后每次make clean都一定能成功。\n自动生成依赖\n在【依赖】这一小节，我们提过修改文件头，观察编译情况。答案是不会更新。如果我们希望修改文件头会引发重新编译，应当如何设计呢？可以做如下修改：\n$(TARGET): $(SOURCE_MAIN) $(LIB_CALC) $(INCS)/calc.h\tg++ -I $(INCS) -L ./lib $(SOURCE_MAIN) -l$(LIB_NAME) -o $(TARGET) \n也就是将头文件直接加入依赖。问题是，大型项目里面维护这样的依赖是很痛苦的事情。这里有一个更好的解决方案：gcc提供了几个选项，可以生成*.d文件，记录依赖关系：\n\n-MMD:生成的文件记录的依赖只包括用户文件。（注意包含头文件）\n-MD：生成的文件记录的依赖包括用户文件和系统文件\n-MP：基于-MMD或-MD之外的选项，为每个依赖添加一个没有依赖的伪目标。可以避免删除头文件时，Makefile因找不到目标来更新依赖报错。\n\n示例如下：\n&gt; g++ -MMD -I ./include -L ./lib/ ./src/main.cpp  -lcalc -o main&gt; cat main.dmain: src/main.cpp include/calc.h&gt; g++ -MD -I ./include -L ./lib/ ./src/main.cpp  -lcalc -o main&gt; cat main.dmain: src/main.cpp /usr/include/stdc-predef.h \\ /usr/include/c++/11/iostream \\ /usr/include/x86_64-linux-gnu/c++/11/bits/c++config.h \\...     # 100 more lines was omitted /usr/include/c++/11/bits/basic_ios.tcc \\ /usr/include/c++/11/bits/ostream.tcc /usr/include/c++/11/istream \\ /usr/include/c++/11/bits/istream.tcc include/calc.h&gt; g++ -MMD -MP -I ./include -L ./lib/ ./src/main.cpp  -lcalc -o main&gt; cat main.dmain: src/main.cpp include/calc.hinclude/calc.h:\n因此，我们可以继续对之前的Makefile做如下修改：\n...# - dependency fileDEPS=$(patsubst ./$(OBJECT_DIR)/%.o,./$(OBJECT_DIR)/%.d,$(OBJECT_CALC))..../$(OBJECT_DIR)/%.o: %.cpp\t@mkdir -p $(dir $@)\tg++ -MMD -MP -c $&lt; -o $@-include: $(DEPS)\ninclude用于将指定文件的内容插入到当前文本中。include 前加了-符号，其作用是指示make 在 include 操作出错时忽略这个错误，并继续执行接下来的操作。出错的原因是*.d尚未产生的时候，这通常在初次编译和clean后编译产生。\n链接先后问题的补充\ng++ -I ./include -L ./lib/ ./src/main.cpp -lcalc -o main    # 编译通过g++ -I ./include -L ./lib/ -lcalc ./src/main.cpp -o main    # 编译不通过，提示找不到引用\n需要注意的是，源文件需要放在库文件之前。这一细节在《深入理解计算机系统》7.6.3 链接器如何使用静态库来解析引用 一节中有所解释，其给出的准则如下：\n\n关于库的一般准则是它们放在命令行的结尾。库之间如果是：\n\n相互独立的：库可以以任何顺序放在命令行中\n不是相互独立的：具体规则是，前面库中产生的引用需要在后面的库中能找到相应定义\n如果需要满足依赖需求，可以在命令行上重复库。如libx.a和liby.a构成循环依赖，可以使用gcc foo.c libx.a liby.a libx.a，但更好的方式是将两者合并成一个单独的存档文件。\n关于a.o-&gt;liba.a-&gt;libb.a-&gt;liba.a-&gt;a.o的依赖，解决方式是gcc -o main a.o liba.a libb.a liba.a而没有a.o，`需要思考一下为什么\n\n\n总结\n参考资料\n\n写给初学者的Makefile入门指南\nLearn C the hard way - 练习28：Makefile 进阶\n\n","categories":["工具链"],"tags":["cpp","Makefile"]},{"title":"【cpp查漏补缺】1-cpp是如何跑起来的？","url":"/2023/10/01/cpp_1_%E9%93%BE%E6%8E%A5/","content":"总概\n本文将介绍cpp从源代码到可执行文件的构建流程。\n预处理\n预处理一步仅仅是简单的文本替换，如包含文件#include，宏展开#define等。预处理的结果是一个经过修改的源代码，通常以.i文件的扩展名保存。你可以尝试下列指令来生成预处理后的文件：\ngcc -E my_program.cpp -o my_program.i\n编译\n编译这一步则是将预处理后的源文件转换成汇编语言，常见的指令集，如x86。\ncpp主流编译器介绍\n\nGCC（GNU Compiler Collection）:准确来说，GCC是由GNU项目开发的编译器套件，包括C，C++，Fortran等。而g++是C++的编译器，gcc是C的编译器（也可以编译C++，但一般用g++代替）\nMinGW（Minimalist GNU for Windows）是一套用于Windows操作系统的开发工具集，它允许开发人员使用GNU工具链在Windows上开发应用程序，包括编译和构建C、C++等编程语言的应用程序。\nMSVC（MicroSoft Visual C++）：是指 Microsoft Visual C++，是 Microsoft 的 C++ 编译器和开发工具集。\nClang：是LLVM（Low-Level Virtual Machine）项目的一部分。Clang 可以在多种操作系统上运行，包括各种Unix-like系统（如Linux和macOS）以及Windows。\nLLVM：是一个开源的编译器基础设施项目，它包括一组模块化的编译器技术和工具，用于开发和优化程序的编译和执行。Clang为LLVM的前端。除此之外，还包括一种虚拟指令集和优化器，允许开发者构建自定义编程语言。\n\n简单总结，主流的编译器为gcc（g++），msvc（中的c++编译器）和clang三种。而LLVM是一个基础框架，可以帮助开发编译器。\n生成汇编代码\ngcc -S -march=x86-64 my_program.cpp -o my_program.S\n注意-march选项用于指定指令集，这里指定的是x86-64。可以选择将x86-64替换成其他选项，如果不存在，gcc会提示非法，并展示所有合法选项。\n汇编\n这一步的任务由汇编器完成：将汇编语言转换成机器代码。除此之外，汇编语言还承担着翻译伪指令的任务。\n&gt; 伪指令：相关指令集并未提供的指令，伪指令的出现原因是为了编译器开发者的方便而发明出来的。最后需要由汇编器完成伪指令翻译的任务。\n链接\n链接是将多个目标文件和库文件组合成一个可执行文件的过程。\n链接器的任务\n链接器的任务主要有：\n\n符号解析：解析各个目标文件中的符号引用，以确定它们对应的符号定义，并建立符号表。这里解释一下符号：符号包括变量名，函数名，类名等。C中有一个关键字extern，即提醒该符号需要在外部文件中查找。另外，如果一个cpp源文件没有包含函数声明而调用了某一函数，那么这一文件是无法通过编译这一步的。如果修过《编译原理》相关课程，这一原因不难想到：无法确定参数列表和返回值，也就无法生成正确的ABI调用。\n地址解析：计算各个符号在可执行文件中的地址。每一个目标文件都是假设从虚拟地址空间的0x00000000开始，多个目标文件组合成一个文件，自然要重新计算地址，也就是下面的重定位步骤。\n重定位：调整目标文件中的代码和数据，使其正确地引用其他目标文件中的符号。\n合并：将所有目标文件合并成为可执行文件。\n\n考虑到本文的出发点，本文主体主要介绍库文件如何创建和链接。本文仅简单介绍了链接的具体步骤。如果对链接的具体细节感兴趣，请参考《深入理解计算机系统》相关章节。\n库文件\n库文件（Library Files）是一种包含已编译二进制代码的文件，它们包含了可重用的程序模块或函数。一般来说，库文件分为静态库和动态库。\n\n静态库\n\n扩展名：.a(Linux)或.lib(Windows)\n在编译时加入可执行文件中\n优点：在于程序执行速度快\n缺点：是增大了可执行文件大小，库文件的更新会引起可执行文件的重新编译\n\n动态库\n\n扩展名：.so(Linux)或.dll(Windows)或.dylib(macOS)\n不会在编译期加入可执行文件，而是在程序运行时加载到内存中\n优点：是多个程序可以共享动态库，有助于节省内存。库文件的更新，不需要重新编译可执行文件，只需替换库文件即可\n缺点：是由于在运行时要加载库文件，程序启动速度较慢。\n\n\n下面介绍静态库和动态库的创建和链接，这里先给出目录：\n这里先给出目录：\n.├── add.cpp├── main.cpp├── mul.cpp├── add.o├── mul.o├── lib│   ├── libcalc.a│   ├── libcalc.so│   ├── libadd.a│   ├── libadd.so│   ├── libmul.a│   └── libmul.so└── main\n# 生成目标文件gcc -o add.o -c add.cpp# 静态库ar -rcs lib/libcalc.a add.o mul.o  # 使用ar(archive)工具创建，其中mylib.a是静态库名gcc -o main main.cpp -L ./lib -lcalc # -L(Library)代表静态库路径，-l(link)是要链接的静态库# 动态库gcc -shared -o libcalc.so add.o mul.o # -shared 表示创建动态库，mylib.so是动态库名gcc -o main main.cpp -L ./lib -lcalc # 与静态库一样ldd main  # 查看依赖关系，通常会发现新加的动态库找不到依赖LD_LIBRARY_PATH=$LD_LIBRARY_PATH:`pwd`/libexport LD_LIBRARY_PATH  # 修改LD_LIBRARY_PATH变量使得能够查找到依赖，注意该方法只对当前终端有效，如果希望能够永久生效，需要修改环境变量ldd main  # 再次查看./main    # 执行\n这里需要强调的是，库名需要用mylib，对应的文件名是libmylib.a或libmylib.so，也就是说，搜索库的时候会在前面加上lib，后面加上.a或.so后缀。然后会优先搜索动态库，其次搜索静态库。\n下面介绍一下多个库链接以及动态库和静态库混合链接的情况。\n# 多个静态库链接gcc -static -o main main.cpp -L ./lib -ladd -lmulgcc -static -o main main.cpp ./lib/libadd.a ./lib/libmul.a# 多个动态库链接gcc -Bdynamic -o main main.cpp -L ./lib -ladd -lmul# 动态库和静态库混合链接gcc main.cpp -L ./lib -Wl,-Bstatic -ladd -Wl,-Bdynamic -lmul  # 第一种方式，先链接静态库，后链接动态库gcc main.cpp -L ./lib -Wl,-Bdynamic -lmul -Wl,-Bstatic -ladd -Wl,-Bdynamic  #第二种方式，先链接动态库，后链接静态库，然后在最后加上`-Wl,-Bdynamic`使用动态连接的命令\n装载\n最后，程序如果需要执行，需要装载器将可执行文件载入内存中，进行执行。其中仍然有一些细节，但不在本章讨论之类。如果希望了解更多的细节，请参考《深入理解计算机系统》。\n总结\n在本文最后，我们再次总结一下gcc常用的选项：\n- -o：指定输出文件\n- -E：对代码仅进行预处理，生成*.i文件\n- -S：生成汇编代码，生成*.S文件\n- -march：指定指令集\n- -c：对源代码进行预处理，编译和汇编，但不链接。（即目标代码.o）\n- -L：指定库所在的目录\n- -I：指定头文件所在目录\n- -l：指定库名，注意对于lib*.so或lib*.a类似的库，有两种写法：-l:lib*.a或-l*\n- -static或-Bstatic：指定静态库链接\n- -Bdynamic：指定动态库链接\n- -Wl,：表示将额外的参数传递给链接器，如-Wl,-Bstatic表明需要接下来链接静态库\n参考资料\n《深入理解计算机系统》\n","categories":["编程语言"],"tags":["cpp","链接"]},{"title":"【cpp查漏补缺】4-gdb的使用","url":"/2023/10/03/cpp_4_gdb%E8%B0%83%E8%AF%95/","content":"为什么要使用GDB，而不是打印大法？\n这是来自于ChatGPT的回答：\n\nGDB（GNU Debugger）是一个功能强大的调试工具，相对于简单地通过打印输出来进行调试，它具有以下优势：\n1.实时交互性： GDB允许您在程序执行期间随时停止程序的执行，检查变量的值，执行特定的命令（如单步执行、继续执行、设置断点等），并进行实时交互。这可以帮助您更深入地了解程序的内部状态，而不仅仅是静态地查看输出。\n2.非侵入性： 使用打印调试时，您需要在代码中插入打印语句，然后重新编译和运行程序。这可能会导致不必要的代码污染和额外的工作。相比之下，GDB允许您在不修改源代码的情况下进行调试。\n3.回溯和堆栈跟踪： GDB可以帮助您查找程序中的错误并跟踪它们的根本原因。它提供了堆栈跟踪功能，允许您查看调用堆栈，找出错误发生的位置，以及如何到达错误点的路径。这通常比简单的打印输出更容易找到问题。\n4.变量监视： GDB允许您监视变量的值，而不需要在代码中插入打印语句。这对于跟踪特定变量的值在程序执行期间如何变化非常有用。\n5.条件断点： GDB允许您设置条件断点，只有当特定条件满足时才会触发断点。这对于跟踪特定情况下的程序行为非常有用。\n6.内存检查： GDB可以帮助您检查程序中的内存错误，如缓冲区溢出、内存泄漏等。这些问题很难通过简单的打印调试来发现。\n7.多线程调试： 如果程序涉及多线程，GDB能够帮助您调试多个线程之间的交互和竞争条件，以及线程的状态和调用堆栈。\n8.脚本支持： GDB支持Python脚本，您可以使用它们来自动化和扩展调试过程。\n总之，GDB相对于简单的打印调试提供了更强大、更灵活和更全面的调试工具，可以帮助您更快速、更有效地发现和解决程序中的问题。当您需要深入了解程序执行过程、排除复杂问题或进行更高级的调试时，GDB通常是一个更好的选择。但对于某些简单问题，打印调试仍然可以是一个有效的快速解决方案。\n\n总的来说，最为方便的地方是只用编译一次代码。打印大法确实舒服，但是每添加一个打印语句就需要重新编译一次项目，这对于大项目来说是非常耗时的。除此之外，脚本也是值得留意的。这为进一步提升效率提供了可能。\nELF文件\n\n待补\n\nGDB基本使用\n调试信息\n首先，编译程序需要开启-g选项，否则不会带上调试信息。\n\n你可以尝试一下readelf -S &lt;目标文件&gt; | grep debug指令，尝试带上-g和不带上-g选项，观察区别。你会发现少了几段。\n同时你也可以试一下file &lt;目标文件&gt;，如果提示是stripped，那么说明符号表信息和调试信息被去除，不能调试。但没有去除不代表可以调试（因为除了debug段还有其他段）。\n\n之后，你可以输入gdb &lt;目标程序&gt;，然后输入run，开始运行程序并进行调试。\n输入设置\n如果需要命令行参数，你可以在run指令后面跟上命令行参数，或者使用set args，后面跟上命令行参数，然后run并执行。注意，set args可以使用重定向，从一个文件读取输入。\n如果需要从标准输入里面读取数据，可以使用set args并重定向。\n进入调试\n直接调试\n# case 1&gt; gdb &lt;obj_file&gt;(gdb) [some command] # You may need to set args, set breakpoints, etc...(gdb) run# case 2&gt; gdb(gdb) file &lt;obj_file&gt;(gdb) [some command] # You may need to set args, set breakpoints, etc...(gdb) run\n调试已运行程序\npidof &lt;obj_file&gt;    # note that &lt;obj_file&gt; is running, using `pidof` to get the pid# note that we need to debug by attaching the process, so we need to sudo or edit /etc/sysctl.d/10-ptrace.conf:kernel.yama.ptrace_scope to zero# case 1:gdb &lt;obj_file&gt; &lt;pid&gt;# case 2:gdb &lt;obj_file&gt; --pid &lt;pid&gt;# case 3:gdb &lt;obj_file&gt;(gdb) file &lt;obj_file&gt;(gdb) attach &lt;pid&gt;\n这里值得提醒的是，第三种方式可以解决已运行程序没有debug段的问题：首先重新编译一个有debug段的程序，然后attach到正在运行的，没有debug段的程序，这样就可以调试已运行的程序。\n断点设置\n(gdb) info breakpoints      # 查看已设置断点相关信息# 注意：方括号表示可选项# break - 普通断点(gdb) break [test.cpp:]9            # 在[test.cpp这个文件的]第 9 行设置断点 (gdb) b func                        # 在【进入】func函数处设置断点，注意break可以简写为b(gdb) break * if a==0               # 在 条件表达式 满足时，设置断点， * 为设置的断点(gdb) condition &lt;num&gt; if a==0       # 为编号为 num 的断点附加条件表达式# # rbreak - 正则表达式断点(gdb) rbreak [test.cpp:]^print*     # (regex break) 为所有[test.cpp中的，]以print开头的函数设置断点(gdb) rbreak [test.cpp:].           # (regex break) 为[test.cpp中的]所有函数设置断点# tbreak - 临时断点，只会生效一次tbreak *                            # 与 break 的用法大同小异# ignore - 到达该断点时忽略掉ignore num 30                       # 跳过编号为 num 的断点 30 次# *watch - 观测某个变量，设置断点，注意程序需要在运行中watch var                           # 用于监视变量 var 的更改awatch var                          # 用于监视变量 var 的读取或写入rwatch var                          # 监视特定内存地址# rwatch *0x12345678# *able - 禁用或启用断点enable / enable [num]               # 启用所有断点/标号为num的断点disable / disable [num]             # 禁用用所有断点/标号为num的断点# clear - 清除断点clear                               # 删除所有breakpointsclear [test.cpp:]func               # 删除[test.cpp中的]函数名为function处的断点clear [test.cpp:]lineNum            # 删除[test.cpp中的]行号为lineNum处的断点# delete - 清除断点delete                              # 删除所有breakpoints,watchpoints和catchpointsdelete lineNum                      # 删除断点号为 lineNum 的断点\n查看变量\n手动查看变量\np [&#x27;file&#x27;::][&#x27;func&#x27;::]var       # 查看[file文件中][func函数中]变量var的值p [&#x27;file&#x27;::][&#x27;func&#x27;::]*var      # 查看[file文件中][func函数中]**指针**var所指向的值p [&#x27;file&#x27;::][&#x27;func&#x27;::]*var@10   # 查看[file文件中][func函数中]**指针**var所指向的值，并向后打印10个值（一般是指针指向了数组用）p [&#x27;file&#x27;::][&#x27;func&#x27;::]*var@a    # 查看[file文件中][func函数中]**指针**var所指向的值，并向后打印a个值p $                             # $表示上一个查看的变量set $index=0                    # 设置变量p b[$index++]# 按照特定格式打印变量：# x d o t 十六/十/八/二进制显示变量，u 十六进制显示无符号整型# a 十六进制 c 字符格式 f 浮点数格式p/x var                         # 按照十六进制打印变量 var\n查看内存内容\n# n内存单元数# f打印格式，在上面提过# u打印的单元长度，有b,h,w,g，分别为一字节，二字节，四字节和八字节# addr 地址x/[n][f][u] addrx/4tb &amp;e        # e 变量 打印内容# 0x7fffffffdbd4:    00000000    00000000    00001000    01000001\n自动显示变量\n# 程序中断时就会显示 display 定义的变量display einfo display            # 查看相应变量信息delete display num      # num 为前面变量前的编号，不带 num 时清除所有disable display numenable display num\n查看寄存器内容\ninfo registers\n源码比对\nl                               # 看 1 - listsize 行l +                             # 看 后面的 listsize 行l -                             # 看 前面的 listsize 行l [file:]lineNum                # 看 [指定文件file的] lineNum 行 附近的 listsize 行l [file:]func                   # 看 [指定文件file的] func 函数 后面的 listsize 行set listsize 20                 # 设置 listsize 行 为 20show listsize                   # 查看 listsizel lineNum_begin,lineNum_end     # 查看 lineNum_begin 到 lineNum_end 的内容，如果缺一个，则向前或向后看 listsize 行\n调试\n基本的步进调试\ngdb gdbStep     # 启动调试# some breakpoints confign 9             # 执行 9 次n               # 执行 1 次s               # 进入函数内部，但函数必须有调试信息finish          # 完成函数调用show step-mode  # step-mode 用来表示碰到没有调试信息函数的行为，默认是跳过，即off。如果需要修改，使用set命令即可si              # 执行一条机器指令（stepi）c               # continue，执行到下一个断点fg              # 也是continueu 29            # until 29，执行到29行停住skip function add   # 跳过add函数skip file calc.cpp  # 跳过calc.cpp中的函数info skip           # 查看跳过信息skip delete [num]skip enable [num]   # 使能编号为num（或所有）的跳点skip disable [num]  \n堆栈调用调试\nbt              # backtrace，可以查看完整的调用堆栈frame 2         # 切换到第2个帧info locals     # 查看当前帧本地变量的值 info args       # 查看当前帧函数参数的值\n指定源码路径\n# 切换至源码移动后的路径dir new_directory# 更改内建变量set substitute-path old_path new_path   # old_path 为原来的路径，new_path 为更改后的路径show substitute-path\n源代码比对\nlayout srclayout splitctrl + x -&gt; A   # 退出ctrl + x -&gt; 1   # 一个窗口ctrl + x -&gt; 2   # 两个窗口\ncore文件调试\n首先，core文件是core dump时产生的。首先需要修改core dump文件的大小：\nulimit -c           # 查看core文件大小，单位为块，一块大小默认为512字节ulimit -c unlimited # 表示无限制ulimit -c 1024      # 表示大小最大为 1024 块\n\ncore文件的具体使用见【熟悉core文件调试】一节。\n\n然后可以这样调试core文件：\ngdb program_file core_file\n一些示例\n熟悉基本操作——并查集\n下面是并查集的代码：\n// dsu.cpp#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;const int DSU_SIZE = 10;vector&lt;int&gt; dsu;int findfa(int a)&#123;    return dsu[a] = a == dsu[a] ? a : findfa(dsu[a]);&#125;void merge(int a, int b)&#123;    int fa = findfa(a);    int fb = findfa(b);    if (fa &lt; fb)&#123;        dsu[fb] = fa;    &#125;    else&#123;        dsu[fa] = fb;    &#125;&#125;void init()&#123;    for (int i = 0; i&lt;DSU_SIZE; i++)&#123;        dsu.push_back(i);    &#125;&#125;int main()&#123;    init();    /*      * input format:     * 1 x y: set the smaller one as bigger one&#x27;s father     * 2 x: query the father of x     */     int op;    while(cin &gt;&gt; op)&#123;        int x, y;        switch (op)        &#123;        case 1:            cin &gt;&gt; x &gt;&gt; y;            merge(x, y);            break;        case 2:            cin &gt;&gt; x;            cout &lt;&lt; findfa(x) &lt;&lt; endl;            break;        default:            break;        &#125;    &#125;&#125;\n下面是测试用例：\n1 4 52 42 51 2 32 22 31 3 42 22 32 42 51 1 52 12 22 32 42 5\n下面是期望输出：\n4422222211111\n现在，我们完成以下任务：\n1.发生merge操作时，观察dsu的变化情况：\n参考答案 \n              \n              gdb dsuset args &lt; input.inwatch dsub mergeb 32        # Note that b 32 is to skip the init(), which would be boring for reallocate vector dsu. You may use disable to disable some breakpoints.info b# repeat below for n timesc       # You may use `c 10` to skip some allocation for vectorfinish  # when stop for entering the function mergep dsu   # show the content of dsu array从这个例子可以看出，gdb所停顿的语句，是暂时没有执行的。这个要注意。其次，在给dsu加上watchpoint后，可以发现在init()函数里面时，都会出现一大堆不可名状的内容。原因在于vector内部的实现，会动态改变vector的大小，并重新分配空间。\n              \n            \n2.每次发生findfa时，查看函数调用帧\n参考答案 \n              \n              b findfaruncbtframe 1info localinfo args也许用递归程序能够更好地体现。比较重要的是info local，可以展现所有的局部变量。\n              \n            \n熟悉core文件调试\n这次是个很简单的例子：\n#include&lt;iostream&gt;using namespace std;int main()&#123;    int* p;    *p = 9961;    return 0;&#125;\n由于作者是边学边做，这里作者遇到了一些问题：作者使用的发行版本为ubuntu22.04，经过查阅资料，发现在/proc/sys/kernel/core_pattern有保存存放地址：|/usr/share/apport/apport -p%p -s%s -c%c -d%d -P%P -u%u -g%g -- %E，经过再进一步的查阅，发现这个地址跟apport这个东西有关。查到的大多数的资料都是修改/etc下面某些文件的配置。我们这里不这样做，而是使用gcore：\ngdb(gdb) file segFault(gdb) run(gdb) gcore                 # 注意只有正在运行的进程可以使用 gcore ，如果run之后发生了段错误，是还会继续运行的，可以使用 gcore 生成 core 文件# (gdb) gcore &lt;target_file&gt;     # 生成的文件名为target_file(gdb) qgdb segFault core.40540     # 第三个参数是 core 文件\n生成core文件并执行后，我们可以看到gdb直接报出错误原因和位置：\nProgram received signal SIGSEGV, Segmentation fault.0x0000555555555175 in main () at segFault.cpp:55           *p = 9961;\n那么core文件是什么呢？core文件就是程序崩溃时的内存快照，记录了崩溃瞬间的内部状态。\n为什么会这样？\n#include &lt;bits/stdc++.h&gt;using namespace std;int* p = nullptr;void g1()&#123;    int a = 3;    p = &amp;a;&#125;void g2()&#123;    int a = 0;&#125;int main()&#123;    g1();    cout &lt;&lt; *p &lt;&lt;endl;    g2();    cout &lt;&lt; *p &lt;&lt;endl;&#125;\n打印的结果是3和0，也许你大概猜到是什么原因了，但我们现在希望能探究一下，这究竟是什么原因。\n参考答案 \n              \n              watch *p        # 这是最快的做法c# Old value = &lt;unreadable&gt;# New value = 3# g1 () at test.cpp:7c# Old value = 3# New value = 0# _dl_runtime_resolve_xsavec () at ../sysdeps/x86_64/dl-trampoline.h:75c# Old value = 0# New value = 21845# 0x00007ffff7d3d09a in std::ostream&amp; std::ostream::_M_insert&lt;long&gt;(long) () from /lib/x86_64-linux-gnu/libstdc++.so.6c# Old value = 21845# New value = 0# _dl_runtime_resolve_xsavec () at ../sysdeps/x86_64/dl-trampoline.h:75c# Old value = 0# New value = 21845# 0x00007ffff7d3d09a in std::ostream&amp; std::ostream::_M_insert&lt;long&gt;(long) () from /lib/x86_64-linux-gnu/libstdc++.so.6c# Continuing.# 0# [Inferior 1 (process 43738) exited normally]不难知道这个p所在的地址被反复发生未知读写的原因是g1函数的栈帧释放后，后面调用的函数又使用了这一块内存，导致内容被反复读写。从调试结果可以看出来，这个值的改变有两个来源：一个是来自../sysdeps/x86_64/dl-trampoline.h，一个是来自/lib/x86_64-linux-gnu/libstdc++.so.6。后者结合输出不难猜出是cout的原因。那前者是什么呢？经过查阅资料，发现是管理动态链接库用的。那么我们继续修改代码如下：int* p = nullptr;void g1()&#123;    int a = 3;    p = &amp;a;&#125;void g2()&#123;    int a = 9;    p = nullptr;&#125;int main()&#123;    g1();    g2();&#125;emm，发现还是有../sysdeps/x86_64/dl-trampoline.h参与，不知道是什么原因，动态链接库是哪里来的？是不是跟加载有关？还是？先把疑问留在这里吧。\n              \n            \n其他\n【编译原理】课程项目——未修复的bug\n参考资料\n大佬们都是怎么用gdb的？或者用吗？ - 张小方的回答(介绍了GDB在大型项目里面的实战？有空看看。)\nGDB调试入门指南\n","categories":["工具链"],"tags":["cpp","gdb"]},{"title":"docker安装","url":"/2023/10/05/docker_1_docker%E5%AE%89%E8%A3%85/","content":"docker在Linux平台上的安装\ncurl -fsSL https://get.docker.com -o get-docker.sh  # 从官网获取安装脚本sudo sh get-docker.sh           # 安装 dockersudo groupadd dockersudo gpasswd -a $USER docker    # 将当前用户加入 docker 用户组，避免总是需要走 rootnewgrp docker                   # 使修改立即生效# reboot                        # 重启（其实我也不知道要重启什么服务）\ndocker根目录迁移\ndocker的根目录在/var/lib/docker下面，这个文件夹增长速度很快，如果根目录没有设置过大的空间，那么迟早会占满，而根目录占满的后果是无法开机。因此，需要对这个目录进行迁移，使用mv即可`。需要注意的是，作者的docker版本为23.0.1，不同版本的解决方案可能不同，请注意。\nsudo service docker stopsudo vim /etc/docker/daemon.json# 在文件里面加入如下内容，其中路径为迁移后的路径#   &#123;#       &quot;data-root&quot;: &quot;/path/to/new/docker-root&quot;#   &#125;sudo service docker startdocker info                     # Docker Root Dir 发生改变即可 \n","categories":["工具链"],"tags":["docker"]},{"title":"【cpp查漏补缺】3-CMake入门","url":"/2023/10/02/cpp_3_CMake%E7%94%A8%E6%B3%95/","content":"前言\n上一节介绍了Makefile的用法，但由于Makefile编写过于复杂，于是CMake工具出现了。CMake用法比Makefile更为简单，其作用是生成Makefile文件。除此之外，CMake允许开发者编写一种平台无关的 CMakeList.txt 文件来定制整个编译流程，然后再根据目标用户的平台进一步生成所需的本地化Makefile和工程文件。本文将介绍CMake的基本用法。\n由于CMake的功能十分强大，本篇文章采用增量式更新的形式（即：作者用到的时候才进行更新）。\nCMake常用函数一览\n这里先扔出一大堆常见函数，先了解其用法，留下一个印象，之后在后面具体实操。\n编译相关\n# 基本信息设置相关cmake_minimum_required(VERSION 2.8) # 规定 CMake 的最低版本project(Demo2 VERSION 0.1)          # 定义 项目 名为Demo2，版本为0.1set(var 1)                          # 变量定义，类似于var=1\n# 添加源代码相关# - 指定 生成目标，第一个是目标名，后面的为源代码add_executable(main main.cc MathFunctions.cc)# - 指定 源代码所在目录，第一个是目录路径，后面的为相应的变量，注意不会递归查找文件# - 使用$&#123;SOURCE&#125;来引用变量SOURCE：add_executable(main $&#123;SOURCE&#125;)aux_source_directory(./src SOURCE)# - 添加子目录，请注意，子目录的CMakeLists.txt因为此函数也会被处理，且在子目录设置的变量会传播到本目录add_subdirectory(./src/calc)\n# 库与链接相关# - 生成链接库：第一个是库名，第二个参数是类型#   默认是STATIC静态链接库，SHARED为动态链接库，后面为源代码add_library(MathFunctions [STATIC] $&#123;DIR_LIB_SRCS&#125;)# - 向目标添加相应库，第一个是目标名，第二个是静态链接库名target_link_libraries(main MathFunctions)\n# 头文件管理相关# - 用于给指定目标设置头文件包含目录，其中第二个参数是权限控制：# - - 1.PUBLIC:允许当前目标和依赖于当前目标的其他目标访问指定的包含目录，且指定目标会加入当前目标的编译过程# - - 2.INTERFACE:包含目录可以被当前目标和依赖于当前目标的其他目标访问，通常使用于库，不会影响库本身的编译过程# - - 3.PRIVATE:包含目录仅在当前目标内可见target_include_directories(main PRIVATE /path/to/include)# - 给整个项目的【所有目标】设置包含目录，一般不推荐使用include_directories(/path/to/include)# - 设置用户可配置的选项，value默认为`OFF`关闭，可选`ON`option(&lt;option_variable&gt; &quot;option_description&quot; [value])\n# 项目配置相关# 根据*.h.in内容，生成*.h的内容，具体用法见后configure_file(&quot;config.h.in&quot; &quot;config.h&quot;)\n安装相关\n这里仅放置一些常见用法，具体见此。\n# install 命令用于处理在CMake中生成的一系列目标# 安装可执行文件和库install(TARGETS main DESTINATION ./bin)install(TARGETS lib DESTINATION ./lib)# 安装头文件install(DIRECTORY include/ DESTINATION .)\n测试相关\n\n待补充\n\n特殊内建变量\n更多内建变量请查阅官方文档，下面列出常用的内建变量。\n\nCMAKE_SOURCE_DIR：处理源代码时，整个项目的最顶层目录\nPROJECT_SOURCE_DIR：处理源代码时，最后一个调用Project()的CMakeLists.txt所在的目录\nCMAKE_CURRENT_SOURCE_DIR：处理源代码时，当前CMakeLists.txt所在目录\nCMAKE_BINARY_DIR：构建过程中，整个项目的顶层目录\nCMAKE_LIBRARY_OUTPUT_DIRECTORY：动态链接库的输出目录，默认目录为pwd\nCMAKE_ARCHIVE_OUTPUT_DIRECTORY：静态链接库的输出目录，默认目录为pwd\nCMAKE_RUNTIME_OUTPUT_DIRECTORY：目标文件的输出目录，默认目录为pwd\nCMAKE_CXX_FLAGS：gcc的编译选项，如-O1 -g -Wall等等附加选项\nCMAKE_CXX_STANDARD：c++的标准\nCMAKE_CXX_STANDARD_REQUIRED：是否强制需要，True或False\n\n举例：我在./build中执行cmake ..，而根目录有CMakeLists.txt，那么CMAKE_SOURCE_DIR为.，CMAKE_BINARY_DIR为./build\n如果还是不明白，可以使用message()函数进行打印。\n初探CMake\n这里沿用我们在上一节中用到的项目，目录结构如下：\n.├── bin├── build                   # CMake 构建的工作目录├── CMakeLists.txt├── config                  # 配置文件目录│   └── conf.h.in├── include│   └── calc.h├── lib└── src    ├── calc    │   ├── add_sub    │   │   ├── add.cpp    │   │   └── sub.cpp    │   ├── CMakeLists.txt    │   └── mul_div    │       ├── div.cpp    │       └── mul.cpp    └── main.cpp\n其次，我们应该如何使用CMake呢？首先创建名为CMakeLists.txt的文件，然后开始编写内容。\n基本信息维护\n首先，我们应该考虑加入基本信息：\ncmake_minimum_required(VERSION 3.0)    # 用于规定 cmake 工具的最低版本project(demo VERSION 0.1)              # 用于定义 project 的名称，注意 demo 是定义的变量名，标识一个 project，后面的 VERSION 标识项目的版本set(CMAKE_CXX_FLAGS   &quot;-g -Wall&quot;)       # 开启警告信息set(CMAKE_CXX_STANDARD 11)             # set 用于设置变量，CMAKE_CXX_STANDARD 用于标识 c++ 标准库的版本，这里设置的是c++11版本set(CMAKE_CXX_STANDARD_REQUIRED True)  # CMAKE_CXX_STANDARD_REQUIRED 表示这个标准是否被需要，这里设置为 True \n这里我们用了几个函数，我们现在来一一介绍。\n\ncmake_minimum_required：对CMake的最小版本要求。值得注意的是，如果版本设置过低，其会因为兼容性的原因而不允许进行构建。\nproject：定义项目名，后面可以跟版本号\nset()：这一函数用于设置变量名。值得注意的是，我们经常使用这个函数修改内建变量，以便于我们项目的构建。\n\n配置选项\n接下来，我们进行一些配置选项的设定。\n# 配置option(ECHO &quot;echo test&quot; OFF)if(ECHO)    message(&quot;Echo on!&quot;)    set(testVar &quot;This is a test&quot;)else()    message(&quot;Echo off!&quot;)    set(testVar &quot;This is a fail&quot;)endif()configure_file(&quot;$&#123;PROJECT_SOURCE_DIR&#125;/config/conf.h.in&quot; &quot;$&#123;PROJECT_SOURCE_DIR&#125;/include/config.h&quot;)\n// $&#123;PROJECT_SOURCE_DIR&#125;/config/conf.h.in#cmakedefine ECHO#cmakedefine testVar &quot;@testVar@&quot; \n其中我们主要用到了configure_file()函数。这个函数提供了一些可能的配置，第一个参数接收配置文件的路径，第二个参数接收生成头文件的路径。\n下面我们重点介绍了一下配置文件的编写。首先，我们使用#cmakedefine，来定义一个可能的宏。注意两个宏定义有所区分：\n&gt; #cmakedefine ECHO：这一个宏定义和option函数对应，需要注意的是option和这里的宏名称要一模一样。option函数有两个取值：ON和OFF。如果为OFF，那么生成的头文件config.h会被替换成/* #undef ECHO */，否则会替换成#define ECHO.\n&gt; #cmakedefine var1 \"@var2@\"：注意这个宏定义，宏名称必须和CMake中设置的变量一样。其次，宏定义的内容可以任意取值。但比较特殊的是用\"@和@\"括起来的CMake定义过的变量。为加以区分，我们用var1和var2来表述。如果var1在CMake中被set或option为ON的话，就会使用var2定义的宏。但如果var2被\"@和@\"括起来的话，var2则会替换成CMake定义过的变量。\n最后还需要留意两个地方：有一个是if-else-endif的控制结构。另外一个是内建变量$&#123;PROJECT_SOURCE_DIR&#125;，这个变量的含义请查阅《特殊内建变量》一节。\n下面是configure_file()函数生成的config.h的内容：\n#define ECHO#define testVar &quot;This is a test&quot;\n在这里，也许#cmakedefine var1 \"@var2@\"的含义还是不太明确。下面给出一些示例。\n# case 1:# var=ON    # by call option()#cmakedefine var &quot;var&quot; =&gt; #define var &quot;var&quot;  # case 2 - 4:# var=OFF, var2=on   # by call option()#cmakedefine var &quot;var&quot; =&gt; /* #undef var */#cmakedefine var &quot;@var@&quot; =&gt; /* #undef var */#cmakedefine var &quot;@var2@&quot; =&gt; /* #undef var */# case 5 - 7:# var=&quot;test&quot;    # by call set()#cmakedefine var &quot;var&quot; =&gt; #define var &quot;var&quot;  #cmakedefine var &quot;@var@&quot; =&gt; #define var &quot;test&quot;  #cmakedefine var &quot;@var2@&quot; =&gt; #define var &quot;&quot;  \n\nQ：为什么我修改了OPTION的定义，但输出的头文件没有变化？\nA：注意到CMake构建后产生的CMakeCache.txt文件，不难找到有：\nECHO:BOOL=OFF\n对的，如果你不清除这个文件，下次构建时就不会依照你的OPTION设定的选项，而是直接从CMakeCache.txt里面读取结果。CMakeCache.txt还包含了众多的内建变量。值得一提的是，set()设定的变量不会被包括进来。\n\n添加源文件\n下一步，我们需要添加源文件了：\n# ./CMakeLists.txt# 添加目录下的所有源代码add_subdirectory(&quot;$&#123;PROJECT_SOURCE_DIR&#125;/src/calc&quot;)\n#./src/calc/CMakeLists.txtaux_source_directory(&quot;$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/add_sub&quot; CALC_SRC_ADSB)aux_source_directory(&quot;$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/mul_div&quot; CALC_SRC_MLDV)add_library(calc STATIC $&#123;CALC_SRC_ADSB&#125; $&#123;CALC_SRC_MLDV&#125;)\n这里需要强调的是，这两份配置来自于两个CMakeLists.txt。第一份来自于根目录，而第二份来自于src/calc。下面介绍三个函数的作用：\n\nadd_subdirectory(path)：将指定路径path加入项目，并根据指定路径的CMakeList.txt继续进行构建。\naux_source_directory(path, var)：将指定目录path下的所有源代码存入变量var中。\n\n这里仍然值得注意的是路径的声明，其中用到了内建变量$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;。其含义仍然查询《特殊内建变量》一节。可以使用message()函数打印。\n添加静态库及动态库\n上面的CMakeists.txt还有一个函数没有介绍，这个函数就是用于构建库的函数。\n\nadd_library(lib_name, [STATIC | SHARED | MODULE], src...)：用于构建目标库，其中第一个参数为库名，第二个参数默认为STATIC，用于构建静态库，后面的参数为源代码。\n\n依赖配置\n下一步是为我们的目标文件添加依赖。看过【cpp查漏补缺】1-cpp是如何跑起来的？的都知道，一个目标文件的构建，需要考虑头文件，依赖库和其他所有源文件。这就是一个目标文件所有的依赖。\nadd_executable(main &quot;$&#123;PROJECT_SOURCE_DIR&#125;/src/main.cpp&quot;)target_include_directories(main PRIVATE &quot;$&#123;PROJECT_SOURCE_DIR&#125;/include&quot;)target_link_libraries(main calc)\n其中我们用到了以下函数：\n\nadd_executable：用于添加可执行目标。后面为所有需要的依赖。这里仅需要src/main.cpp\ntarget_include_directories：用于向目标添加头文件依赖所在的目录，这里是./include/这个目录。关于PUBLIC，PRIVATE和INTERFACE的权限控制符的含义见《编译相关》一节。\ntarget_link_libraries：用于向目标添加库文件依赖，这里是calc这个库，在src/calc/CMakeLists/txt中定义的库。\n\n安装配置\n安装配置的作用是将整个CMake构建过程的中间目标，通过make install的方式输出到指定路径。增加配置如下：\ninstall(TARGETS calc DESTINATION &quot;$&#123;PROJECT_SOURCE_DIR&#125;/lib&quot;)install(TARGETS main DESTINATION &quot;$&#123;PROJECT_SOURCE_DIR&#125;/bin&quot;)\n这里仅仅是一些简单示例，更多用法请查看文档\n如何启动\n首先，由于CMake构建过程中会产生很多中间文件。因此，我们在build文件夹下进行构建：\ncd buildcmake ..\n构建完成后，我们发现有很多中间文件，其中生成了makefile。我们进行make：\nmake\n执行后的项目结构如下所示：\n.├── bin├── build│   ├── CMakeCache.txt│   ├── CMakeFiles│   │   ├── 3.22.1│   │   │   ├── ...│   │   │   ├── ...│   │   │   └── ... (omitted)│   │   ├── main.dir│   │   │   ├── ...│   │   │   └── ... (omitted)│   │   ├── ...│   │   └── ... (omitted)│   ├── ...│   ├── main│   ├── Makefile│   └── src│       └── calc│           ├── CMakeFiles│           │   ├── ...│           │   ├── ...│           │   └── ... (omitted)│           ├── ...│           ├── libcalc.a│           ├── Makefile│           └── ... (omitted)├── clean_script.sh├── CMakeLists.txt├── config│   └── conf.h.in├── include│   ├── calc.h│   └── config.h├── lib└── src    ├── calc    │   ├── add_sub    │   │   ├── add.cpp    │   │   └── sub.cpp    │   ├── CMakeLists.txt    │   └── mul_div    │       ├── div.cpp    │       └── mul.cpp    └── main.cpp\n可以发现，相应的目标文件都在build里面，和其他的中间文件混在一起很乱。\n如果我们在build目录下执行make install呢？由于我们进行了安装配置，可以发现main和libcalc.a分别出现在了bin和lib下面，这达到了我们的预期。\n实际上，我们可以修改一些内建变量，达到执行make就能将相应文件放到我们想要放的地方中。在根目录下的CMakeLists.txt的前面几行加入如下内容：\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;/bin)   # 可执行文件所在位置set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY $&#123;PROJECT_SOURCE_DIR&#125;/lib)   # 静态库文件(archive)所在位置\n再次执行make，可以发现相应目标文件都在根目录的bin和lib下了。但与此同时，build下面就不会出现这些目标文件了。\n\n最后一种做法是作者在《编译原理》课程上看到的框架提供的做法，不清楚是否有潜在危害。\n\n测试\n\n待补充\n\nCMake测试\nCMake宏\n项目迁移\n\n待补充\n\n参考资料\n\nCMake tutorial\n15分钟学习CMake脚本\nC++入门后需要学习下Cmake吗？ - 马小刀的回答\n全网最细的CMake教程！(强烈建议收藏)\n\n","categories":["工具链"],"tags":["cpp","CMake"]},{"title":"git快速入门","url":"/2022/07/18/git_gitStarter/","content":"前言的前言\n由于关于Git的介绍实在过多，反复记录意义不大，因此本文将不再作大幅度更新（大概弃坑了，可能会介绍一些不太常见的内容，例如钩子函数）。下面是一些学习资料的推荐：\n\nLearning Git：强烈推荐！交互式的操作和可视化的界面，仅需三个小时就可以完全掌握Git。\n我看谁还不懂 Git ！(万字干货) 知乎上刷到的一篇博文，很详细细致，有大量绘制的图。\nPro Git：有大佬推荐，貌似很出名，但是本人没有看过。\n\n本文以后主要更新一些作者在实践过程中遇到的各种各样的问题（如100MB文件提交限制），以及一些进阶内容（说白了就是很少用的内容），如钩子函数。\n在学习过程中，带着问题更有效，下面是作者提出的一些基础问题，在学习过程中需要思考：\n\nGit是什么？Github又是什么？两者有什么关系？你还知道哪些代码托管平台和版本管理工具？\n本地仓库是如何管理的？工作区，暂存区等概念是什么？\n本地仓库和远程仓库的概念是什么？两者间如何交互？\n本地仓库的分支是如何管理的？有哪些常见操作？这些操作需要注意什么？什么时候会出现数据丢失的风险？\n为什么不建议在仓库中提交Word，图片等文件？\n\n希望在学习过程中思考这些问题。当然，如果仅仅是刚开始入门，那么仅需参考本文的Git及代码托管平台初步使用一章即可。\n【挖坑】后面想探索一下：\n- 基于github的多人协作，重点介绍分支保护，PR，issue等等，预计会开个新帖\n- github action这些玩意是什么东西（跟博客有关），以及Wiki，release等等，预计会开个新贴介绍。\n前言\n如果：\n\n您是git用户的新手，希望能快速上手使用，那么阅读完Git及代码托管平台初步使用章节即可\n如果您遇到了任何问题，请移步疑难杂症记录，那里记录了作者使用git以来的各种问题，希望能帮助倒您\n如果您需要进行多人协作的开发，请移步多人协作\n如果您需要查找常用指令的各种用法及作用，请移步常见操作记录，并善于运用git xxx --help指令（其中xxx是您想使用的操作）。\n如果你需要熟练操作，可以尝试这个！\n\nGit及代码托管平台初步使用\n本节介绍单人从git开始到拥有一个完整的仓库，并如何将本地数据同步上去。\n\nGit：版本控制工具；GitHub/Gitee：社交代码托管平台\n因此将代码托管到github和gitee的步骤大同小异。本文以github为例，介绍如何使用Git及托管平台。\n\n准备步骤\n这里以Windows11操作系统为例，介绍本地git的环境配置1。\n开始之前，您需要：\n\n注册github/gitee账号，并牢记昵称和邮箱，并不要轻易更改昵称和密码（修改后会有坑）\n下载git工具，作者用的是2.37.3版本，安装过程根据提示即可，初学者可以一直点next。\n[可选]下载Git的图形化操作工具TortoiseGit\n\n以上操作执行后，就可以配置环境了。首先右键打开菜单，找到Git Bash Here选项。打开终端，输入以下内容（引号内内容替换为注册账号时所用信息即可，分别为昵称和邮箱）：\ngit config --global user.name &quot;yourName&quot;              # example: git config --global user.name &quot;spiritTrance&quot;git config --global user.email &quot;yourEmail@email.com&quot;  # example: git config --global user.name &quot;15781xxxxx@qq.com&quot;\n之后在本地创建ssh key：\nssh-keygen -t rsa -C &quot;yourEmail@email.com&quot;      # example: ssh-keygen -t rsa -C &quot;15781xxxxx@email.com&quot;\n之后连续敲击三次回车键后会生成两个隐藏文件，在windows11环境下（其他环境需要留意cmd的提示）放置在该路径：C:\\Users\\&lt;你的电脑用户名&gt;\\.ssh，在.ssh下找到 id_rsa.pub文件，复制里面的内容。然后登录托管平台，进入设置项目，在侧边栏找到SSH and GPG keys，添加SSH key，在Key里面粘贴 id_rsa.pub文件里的内容，Title内容可以随意填写。\n\n到现在为止，你已经在本地部署好了git，已经能够离线进行版本控制了。\n仓库创建及同步\n这一小节主要介绍本地仓库和远程仓库的同步。\n托管平台上的仓库创建\n\n登录github，点击个人头像，在下拉菜单里找到Your repositories，这里就是你的远程仓库了。\n然后找到New，开始创建你的仓库，按照其要求填写即可。（注意：在github上，如果勾选需要README.md，则分支名为main，否则为master）\n关于license协议的选择，请看这篇文章。一般来说初学者选择MIT协议，可以规避很多问题。\n\n本地仓库创建与远程仓库推送\n当github/gitee上的远程仓库创建好后，如果里面没有任何文件，那么github/gitee会出现以下提示帮助你快速上手。所以你可以在你的工作目录下面打开命令行，输入以下内容，对应含义请见注释.\n在命令行创建新的仓库的步骤：\necho &quot;# blogPostArchive&quot; &gt;&gt; README.md   # 添加README.md文件，里面一般用于介绍你的仓库git init            # 初始化本地仓库git add README.md   # 将README.md文件加入本地仓库的暂存区，可以加`.`符号表示加入当前工作目录下的所有文件（但.gitignore文件中设置的文件除外）git commit -m &quot;first commit&quot;  # 将暂存区里面的内容提交，并加上&quot;first commit&quot;的注释git branch -M main # 命名分支名为main（默认为master）git remote add origin git@github.com:[你的用户名]/[你的远程仓库名].git # 操作远程仓库，后续推送需要 例：git remote add origin git@github.com:spiritTrance/gitlearning.gitgit push -u origin main # -u 设置推送流，origin是远程仓库别名，main是本地仓库分支名（特别注意本地分支名是main还是master）\n在命令行推送本地已有的仓库的步骤：\ngit remote add origin git@github.com:spiritTrance/gitlearning.gitgit branch -M maingit push -u origin main\n值得一提的是，一个仓库一般都会有README.md文件和.gitignore文件2:\n\n  \n    \n      提示\n\n    \n    \n      \nREADME.md文件是github上绝大多数都会有的文件。文件名为README.md的文件会在代码托管平台上，直接在页面中显示。这一文件的作用主要是介绍当前仓库的内容。\n.gitignore文件的作用是在远程推送的时候，避免某些文件被纳入版本管理。一般来说，某些涉及机密的文件，以及单个文件大小过大（50MB会出现警告，100MB会被禁止推送，需要使用git LFS工具）的文件是需要加进来的，以及.gitignore文件本身。.gitignore文件的编写示例如下：\n# *.a             表示忽略所有 .a 结尾的文件# !lib.a          表示但lib.a除外# /TODO           表示仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO# build/          表示忽略 build/目录下的所有文件，过滤整个build文件夹；# doc/*.txt       表示会忽略doc/notes.txt但不包括 doc/server/arch.txt # bin/:           表示忽略当前路径下的bin文件夹，该文件夹下的所有内容都会被忽略，不忽略 bin 文件# /bin:           表示忽略根目录下的bin文件# /*.c:           表示忽略cat.c，不忽略 build/cat.c# debug/*.obj:    表示忽略debug/io.obj，不忽略 debug/common/io.obj和tools/debug/io.obj# **/foo:         表示忽略/foo,a/foo,a/b/foo等# a/**/b:         表示忽略a/b, a/x/b,a/x/y/b等# !/bin/run.sh    表示不忽略bin目录下的run.sh文件# *.log:          表示忽略所有 .log 文件# config.php:     表示忽略当前路径的 config.php 文件# /mtk/           表示过滤整个文件夹# *.zip           表示过滤所有.zip文件# /mtk/do.c       表示过滤某个具体文件\n但需要注意的是，如果你是在途中加入的.gitignore，之前的提交信息并不会删除，删除之前的提交记录是一件比较麻烦的事情。因此，请务必养成写.gitignore的好习惯，尤其是可能产生的大文件。\n扩展阅读:git是如何实现版本管理的？git add, git commit时发生了什么？\n\n\n    \n  \n本地仓库及远处仓库更新\n如果工作目录下的文件有更新，需要进行版本管理，则操作如下：\ngit add . # 将工作目录下的所有文件加入本地git repo的缓存区git commit -m &quot;update Message&quot;  # 提交更新信息git push  # 推送到远程仓库的main分支\n如果上面的三个操作都会了，其实已经可以满足日常单人开发的使用了。但如果是多人开发，建议按照如下步骤进行。\ngit fetch # 将代码托管平台上的文件fetch下来git add . # 将工作目录下的所有文件加入本地git repo的缓存区git commit -m &quot;update Message&quot;  # 提交更新信息git merge # 合并git push origin main  # 推送到远程仓库的main分支\n下面是一些QA：\n\n\nQ1:为什么需要用git fetch而不是git pull?\nA1:原因见此\nQ2:git merge出现冲突如何解决?\nA2:因为各种原因（常见于多人协作），merge后在上面最后一条执行完后会出现冲突问题，如下图所示：\n\n\n此时你需要打开冲突的文件（本地文件图标右下角为黄色感叹号的文件）：\n\n可以看到图中有两个来源的信息，需要将带有&lt;&lt;&gt;&gt;的一行和===的一行删除，可以选择两个来源都保留，或者只保留一方，或者都删掉，均可。之后重新走一遍add,commit,merge,push的流程即可。（注意按顺序）\n\n\nQ3:如果我不使用代码托管平台呢？\nA3:那么你只需要add, commit两步操作即可。\nQ4:为什么要fetch/pull呢？\nA4:多人协作的场景下，有可能你的同事进行了更新而你没有，那么要先同步。如果是单人的话，那么可以直接push，不需要fetch和merge进行同步。\n\n\ngit基础概念介绍\n本地仓库\n因为git还涉及其他奇奇怪怪的操作，可能会涉及各种各样的问题，因此这里先介绍一些有关git设计的基础概念。\n这里借用菜鸟教程的一张图，画的很好：\n\n简单来说，Git管理有两个区域：工作区（就是你能看到的目录）以及版本库（.git下面的记录），而版本库里维护了很多信息，主要包括：暂存区，目录树和对象库。其中：\n\n\n暂存区：暂时存放更新内容。使用add指令，会将工作区内的内容加入暂存区，更新内容会加入对象库中。而rm --cached则是add的逆过程，会将未提交的内容从暂存区移除（但工作区不受影响）\n版本库：commit指令会将本次更新内容从暂存区加入版本库。而checkout指令会将某个版本的内容恢复到工作区中（注意：此时会清除工作区中未加入到暂存区的内容）。同时commit后，HEAD指针产生变化，移动到当前commit的内容。\n对象库：维护更新内容。\n\n\n因此，下面简单介绍一下常用指令的用法：\n\n\nadd：将更新内容加入到暂存区中。\ncommit：将暂存区中更新的内容提交到目录树中进行维护。\nrm --cached  ：从暂存区删除未提交文件，工作区则不做出改变。\ncheckout：危险操作，会清除工作区未添加到暂存区的内容。\nreset：暂存区的目录树会被重写，被 master 分支指向（HEAD）的目录树所替换，但是工作区不受影响。\nlog：查看历史提交记录。\n\n\n远程仓库\n那么现在考虑托管平台，又该如何做呢？还是借用菜鸟教程的图：\n\n是的，与托管平台之间的交互很多，主要有git pull，git push等，这是最常见的交互方式。但如果需要push。想必之前你已经弄好了一个仓库，那么，我们试一下如下指令？\ngit remote -v # origin  git@github.com:spiritTrance/blogPostArchive.git (fetch)# origin  git@github.com:spiritTrance/blogPostArchive.git (push)git branch# * mastergit branch -a# * master#   remotes/origin/mastergit branch -avv  # * master 9af8fd2 [origin/master] 博客文章存档# remotes/origin/master 9af8fd2 博客文章存档\n那么这几条语句可以帮助你看到，远程仓库是git@github.com:spiritTrance/blogPostArchive.git，其别名是origin。而git branch加入-a选项后，可以同时看到本地和远程仓库的分支。对的，本地有一堆分支，远程仓库同样可以有一堆分支。使用-vv/-avv选项，可以看到分支的最后一条提交记录。值得注意的是，master一行后面的方括号，标注了对应的远程分支。因此，不难猜出，本地的一个分支和远程的一个分支其实是有对应关系的。那么我们应当如何设置这个对应关系呢？回到之前的教程，我们可以发现如下语句：\ngit branch -M main git remote add origin git@github.com:[你的用户名]/[你的远程仓库名].git git push -u origin main \n其中\n- 第一条语句的作用是将当前分支的名称改成main\n- 第二条语句的仓库是加入远程仓库git@github.com:[你的用户名]/[你的远程仓库名].git，别名为origin\n- 第三条语句是关键：-u全名为--set-upstream，这个指令用来设置推送流，将本地的main分支和远程的main分支（没有则自动创建）关联起来（当然，你可以按照&lt;localbranch&gt;:&lt;remotebranch&gt;的格式指定远程分支）。之后使用git push，会默认按照这个上游分支推送。\n你可以修改推送流，推送到不同的远程分支，甚至将本地仓库与多个远程仓库进行关联。\n你可以将本次推送推送到不同分支：git push origin &lt;localBranch&gt;:&lt;remoteBranch&gt;，如果远程分支不存在，甚至会创建，因此又引出了另外一个议题——分支保护。这在多人协作中防铸币队友特别好用。另外，你也可以直接删除远程仓库的分支：git push &lt;remote&gt; --delete &lt;branchname&gt;.\n总结一下，本小节你应该知道如下内容：\ngit remote add &lt;remoteAlias&gt; git@github.com:[你的用户名]/[你的远程仓库名].git # 使用本指令添加远程仓库，别名为remoteAlias（一般是origin）git push --set-upstream &lt;remoteAlias&gt; &lt;localBranch&gt;:&lt;remoteBranch&gt; # 使用本指令设置上游分支并推送到指定仓库的指定分支git push &lt;remoteAlias&gt; --delete &lt;remoteBranch&gt; # 删除远程指定分支\n分支及版本管理\n考虑到项目开发可能存在多个版本，因此git有分支模型，最常见的是maim分支和dev分支，前者是发行版本，后者是开发版本。当然，在多人协作中也更为常见。下面介绍一下git的基本原理，以及与分支管理相关的操作。\nVsCode扩展 - Git Graph\nGit Graph扩展可以清晰地展示git仓库的分支的历史提交信息和合并情况，因此强烈建议安装使用。\n\n安装好后，按下ctrl+shift+p，选择Git Graph: View Git Graph，即可查看当前目录的分支信息。\ncheckout指令\ncheckout指令可以干什么呢？可以做的事情很多：（创建并）切换分支，版本回滚。但实际上，如果认真看了【分支管理】小节的菜鸟教程和HEAD的超链接的介绍，稍加思考就可以知道，checkout本身的作用就是恢复到对应的提交记录。下面在cmd中输入git checkout -help可以查看基本用法：\nusage: git checkout [&lt;options&gt;] &lt;branch&gt;   or: git checkout [&lt;options&gt;] [&lt;branch&gt;] -- &lt;file&gt;...\n其中：\n\n&lt;options&gt;的作用请输入git checkout -help自行查看，其中常用的选项是-b。\n&lt;branch&gt;的作用是指定某个分支，或者记录。例如，可以选分支名master或dev或origin，可以是某次提交记录：使用git log并找到对应的提交记录进行恢复，如某个SHA值9653625095299ae3f21ec254fc41f3588abbabba，又如HEAD（即最后一次commit的记录）或是HEAD^（HEAD指向的记录的前一条记录）。\n-- &lt;file&gt;就是指定某个文件进行恢复。\n\n值得提醒的是，checkout指令会覆盖到工作区的内容（也就是你当前看到的目录的内容），请务必小心使用。\n下面展示新建分支并切换的用法：\n首先，我们创建好一个文档后，使用add指令和commit指令，提交刚创建的文档。右边的Git Graph就显示了当前的提交记录。\n\n此时，我们可以在命令行里面键入如下指令：\ngit checkout -b dev # 创建dev分支并切换到dev分支# 这条指令相当于git branch dev; git checkout dev指令.（branch指令将在后面介绍）。\n之后修改文件，再次执行add,commit 操作即可。\n第二个用法是版本回滚，再次强调会覆盖工作区的内容。简单罗列下指令：\ngit checkout .    # 【工作区】的内容回滚到最后一次提交的内容git checkout HEAD . # 同上\n另外稍微提醒一下，HEAD的变化在切换分支的时候也会发生。\nbranch指令\ngit branch       # 罗列所有分支git branch bName # 创建一个名为bName的分支，后续通过checkout指令切换git branch -d bName # 删除一个名为bName的分支\nmerge指令\nmerge的用途在于合并分支。现在我有master分支和dev分支，其中dev分支由master的基础上创建出来，其中master有README.md文件，dev分支有README.md文件。此时如果在master分支创建test1.txt文件，dev分支创建test2.txt文件，那么在master分支merge的时候，两个文件都会合并进入master分支。\ngit merge dev     # 在master分支上，将dev分支合并进来git branch -d dev # 删除dev分支\n但如果在创建分支后，在两个分支上都对同一个文件进行了修改，那么对应文件会标出冲突部分，按照提示修改，然后重新add，commit，merge即可。\nrebase指令\n（待补）\nrebase与merge谁更好？（见评论区）\n评论区高度赞同的回答——关于rebase和merge的比较\nreset指令\n用途：回退版本，取消缓存区的内容，参考教程\ntag指令\n使用场景：到达一个重要阶段使用。使用参考\nstash指令\n使用场景：你正在dev分支工作，然后突然告知release分支出现问题需要马上修复，然而你dev上已经做了修改但还不能commit，此时你希望能先暂存你的进度，那么可以考虑stash指令。使用参考\ncherry-pick\n远程仓库操作\n远程仓库与本地仓库管理区别在哪？\n图片参考\nhttps://www.runoob.com/w3cnote/git-guide.html\npull,push与fetch指令\n进阶内容\n钩子函数\n钩子函数的位置在.git/hooks下面，钩子函数是一个特别强大的工具，可以帮助你在进行对应操作之前进行检查，如大文件提交问题可以通过设置钩子函数解决。\n创建.git/hooks/pre-commit，在里面写入如下内容：\n#!/bin/sh# pre-commit hook checking for files larger than 100MB# Set the file size limitLIMIT=$((100 * 2**20))  # 100MB in bytesisFlag=0# Check staged filesgit diff --cached --name-only -z | while IFS= read -r -d &#x27;&#x27; FILE; do    # echo &quot;Checking file: $FILE&quot;    if [ -f &quot;$FILE&quot; ] &amp;&amp; [ &quot;$(wc -c &lt;&quot;$FILE&quot;)&quot; -gt &quot;$LIMIT&quot; ]; then        if [ &quot;$isFlag&quot; -eq 0 ]; then            echo -e &quot;\\n# [Warn] At $(date +&quot;%Y-%m-%d %H:%M:%S&quot;): Big size file founded (more than 100MB). They are unstaged and recorded in \\`.gitignore\\`.&quot; &gt;&gt; .gitignore            isFlag=1        fi        echo &quot;Warning: File $FILE is larger than 100MB, it is removed from staged area&quot;        git reset &quot;$FILE&quot;        echo &quot;$FILE&quot; &gt;&gt; .gitignore    fidone\n即可在git commit操作时执行pre-commit脚本。其他函数详见.git/hooks/*.sample里面的注释。\n常见操作记录\n内建图形化界面-gitk\n在有git仓库的项目下，在bash中输入gitk即可进入图形化界面。\n疑难杂症记录\nfootprint更改\n\ngit push时有警告，说footprint更改，还需要输入密码，但是无论怎样都是错的，怎么办？\n看这个博客解决解决Github拒绝授权问题Permission denied, please try again\n\n大文件的提交\n\n注意github不允许单个文件超过100MB的文件提交，所以请设置好 .gitignore文件\n如果不慎提交大文件，一个解决方法使用Git lfs（还不会，貌似要收费，不推荐），另外一个是将大文件的提交记录从本地删除，再重新提交，具体操作步骤如下：\ngit loggit reset &lt;hashcode&gt;\n其中git log 是查看历史提交记录（所以git commit填写的内容十分重要！！），查看后，使用git reset逐步取消提交记录，直到提交大文件的那次记录的前一条记录（包含前一条），之后重新add,commit,push即可\n\ndetached HEAD\n在学习checkout的时候，不小心整出了这样一个状态：\n\nNote: switching to 'HEAD^'.\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\ngit switch -c \nOr undo this operation with:\ngit switch -\nTurn off this advice by setting config variable advice.detachedHead to false\n\n原因分析：输入git checkout HEAD^即可复现，原因未知。git checkout HEAD^ .就不会出现这个状况，指定下文件？\n恢复方法：输入git branch可以查看分支，然后当前分支是detached xxxx，然后可以在该游离的“分支”上使用git merge合并分支，以保证提交不会被gc掉。\n扩展阅读\n\n猴子都能懂的git入门\nPro Git（GitHub员工写的书，从原理解释）\n\n\n\n\ngithub简明资料↩︎\ngit提交push过滤规则.gitignore(本文为CSDN博主「HKYAOYAOLING」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。)↩︎\n\n\n","categories":["工具链"],"tags":["git"]},{"title":"Hexo快速入门","url":"/2022/08/26/hexo_HexoStarter/","content":"参考资料：①hexo框架使用入门②主题particleX的配置和使用③redefine主题文档\n常用指令：\nhexo g  //生成页面hexo d  //部署hexo s  //本地预览hexo g -d  //生成并部署\nQ： particleX主题的代码块有行号，怎么办？\nA： 看particleX theme的介绍，关闭highlight即可\nQ： 头像和背景的URL？\nA： 需要图床。作者使用的是铁锈图床（能用就行）（不知道放进file里面上传url可不可以，file见后面）\nQ：Hexo放置Pdf？\nA：参考：【如何在网页中实现pdf在线预览】10分钟学会如何利用Hexo博客上传本地pdf文件并在线预览pdf_白马金羁侠少年的博客-CSDN博客\n\n后面编辑文档的时候看到了，这里说一下，基于hexo生成的网页全部放在public文件夹下面的，路径索引根据这个来就好。但是由于之前使用的是particleX主题，索引方式为何这么勾把奇怪，我也不知道。\n\n\n注意：对于particleX，由于生成的文件编排方式不同，文件路径应该为：\n../../../../file/&#123;pdfName&#125;.pdf\n注意file放置在写博客的source文件夹下，与_posts同级\n\n每个markdown应该带的信息头：\n---author:- SpiritTrancedate: 2022-08-26title: 计算机网络学习笔记description:- 计算机网络学习笔记tags:- 计算机网络categories:- 学校课程---\nQ：对于Tags页面，Categories页面没有内容？\nA：第一步：在根目录打开Git Bash，输入以下内容：\nhexo new page &quot;tags&quot;\n第二步：在source里面发现tags目录，进入，打开index.md，文件头输入以下信息即可：\n---title: &quot;tags&quot;type: &quot;tags&quot;layout: &quot;tags&quot;---\ncategories同理，about可以自己编写内容进行自我介绍。\nQ：gitalk搭建？\nA：参考以下网址：\nhttps://zhuanlan.zhihu.com/p/141078552\nhttps://blog.csdn.net/m0_46916422/article/details/124065600\n最后参考particleX作者的blog，在_config.yml里面配置好即可。\n\n踩坑1：未找到相关issue，联系@xxx\n\n首先确保OAuth application的homepage URL是存放评论的仓库的URL，callback URL是blog网址的URL。\n然后注意调用Gitalk的方法中，repo是仓库名而不是URL。\n这些确保正确后，点击登录（本地localhost貌似不行，要部署上去），按照提示授权即可。\n如果主题用的是particleX的话，注意_config.yml的repo是存放评论的仓库名（而非URL），生成Gitalk评论区的代码在source目录的script.ejs中，可以查看原始调用方式\n\n\n踩坑2：Error: Vaildation Failed.\n\n部分文章评论区出现这种报错。一般情况是Gitalk的id长度超过50，所以避免id长度超过50即可。\n一般id用的是pathname，所以保证pathname不要过长，别用中文，只含数字字母下划线即可，没有必要使用md5压缩长度（网上搜到解决办法大多是MD5压缩长度）。\nparticleX使用的id也是pathname，即文件名，即_posts下的文件名称而非文件中的title文章名称，注意particleX的路径还会添加日期，所以文件长度不能超过39（前面多了2022/08/26，url里面能看到）\n\n\nQ：latex公式渲染？\nA：首先下载pandoc，然后再做相应配置（参考particleX作者的博客，略，后面再改）\nQ：电脑不同，迁移博客？\nA：作者是直接粗暴地把文件夹复制到新电脑，然后按照创建博客的步骤搭一遍，然后source（博客）直接复制过来覆盖，根目录的config.yml复制过来覆盖，以及theme（因为自己改过）进行覆盖。\n\n  \n    \n      提示\n\n    \n    \n      后面更新博客的时候稍微清楚了一点，你完全可以在根目录下创建git仓库，然后利用.gitignore过滤掉其他文件，只留source文件夹，存档即可。实际上：\nhexo g      # 根据theme的定制生成public文件夹hexo clean  # 删除public文件夹\n图片的寻址也是在public文件夹下面进行的。\n\n    \n  \nQ：如何插入图片？\nA：参考这篇博客\n\n后面编辑文档的时候看到了，particleX使用的方式是testpic.png，然后在source/_posts/hexoStarter/testpic.png放置图片，后面使用的redefine主题，直接根目录索引就可以了= =\n\nQ：如何插入pdf文档？\nA：首先需要安装插件，然后在source文件夹下创建文件夹file，然后在正文输入：\n&#123;% pdf ../../../../file/fileName.pdf %&#125;[笔记地址](../../../../file/fileName.pdf)\nQ：hexo cheatsheet(一键安装)\nA：\nsudo apt-get install npm -y# sudo apt-get install nodejs -y        # 注意一下版本问题，不要太早sudo npm install -g hexo-cli -y       # hexo框架sudo apt install pandoc -ysudo npm uninstall hexo-renderer-markded --save -ysudo npm install hexo-asset-image --save -ysudo npm install hexo-deployer-git --save -ysudo npm install hexo-pdf -S -ysudo npm install hexo-renderer-pandoc -ysudo npm install --save hexo-deployer-git -ysudo npm install --save hexo-asset-image -ysudo npm install --save hexo-math -ysudo npm uninstall --save hexo-renderer-marked -ysudo npm install --save hexo-renderer-pandoc -ysudo npm install --save hexo-filter-flowchart -y\n","categories":["博客撰写"],"tags":["Hexo"]},{"title":"杂项","url":"/2022/09/12/misc/","content":"深度学习\n\npytorch: CUDA和cuDNN安装\nanaconda环境变量配置\nvscode+anaconda+pylint找不到库\nPython › Analysis: Extra Paths:加上anaconda3-&gt;Libs-&gt;site-packages，以及envs-&gt;Libs-.site-packages的内容\n\n","categories":["Misc"],"tags":["Misc"]},{"title":"正则表达式学习","url":"/2023/08/17/linux_%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%AD%A6%E4%B9%A0/","content":"参考教程\n正则表达式 - 语法\n说明：速查速记，学习参考网址已足够。可以配合练习网址食用。\n正则表达式格式\n/pattern/flag，其中pattern为模式串，flag为修饰符\n普通字符\n不赘述。a-zA-Z0-9_\n\n\\s 匹配空白符（包括换行），\\S 匹配非空白符\n\\w 匹配字母数字下划线（等价于匹配程序的identifier）\n\\d 匹配所有数字字符\n. 代表任意字符（不包括换行符，除非修饰符为s）\n\n非打印字符\n重要的就\\n\\r这两个换行符及制表符\\t\n限定符\n作用是指定前面的字表达式出现几次：\n\n?：匹配0-1次，等价于&#123;0,1&#125;\n*：匹配0到无数次，等价于&#123;0,&#125;\n+：匹配1到无数次，等价于&#123;1,&#125;\n&#123;m,n&#125;：匹配m到n次，m和n可以没有，逗号表范围\n&#123;m&#125;：匹配m次\n\n贪婪与非贪婪匹配\n由?控制，例：对于待匹配串&lt;h1&gt;str&lt;h1&gt;，模式串&lt;.*?&gt;为非贪婪模式，仅匹配&lt;h1&gt;，如果为&lt;.*&gt;，默认为贪婪模式，匹配&lt;h1&gt;str&lt;h1&gt;\n定位符\n\n^：匹配字符串开始位置，注意修饰符如果是m，则还会匹配换行符\\n\\r后的位置\n$：匹配字符串结束位置，特性同^\n\\b：匹配单词边界（空格）\n\\B：匹配非边界\n例：\\Bcha 可以匹配到achaa，但chapter不行，但\\bcha反之。^cha可以匹配chapter。\n\n选择\n使用|即可。\n例：a(a|b)a可以匹配aba和aaa.\n正向预查和负向预查\n记忆：带&lt;为负向预查，否则为正向。\n\nexp1(?=exp2)：查找 exp2 前面的 exp1\n(?&lt;=exp2)exp1：查找 exp2 后面的 exp1\nexp1(?!exp2)：查找后面不是 exp2 的 exp1\n(?&lt;!exp2)exp1：查找前面不是 exp2 的 exp1\n\n反向引用\n不是很了解，暂略\n修饰符\n\ni：不区分大小写\ng：全局匹配\nm：多行匹配，对^和$有影响\ns：特殊字符圆点.包含换行符\\n\n\n","categories":["linux"],"tags":["正则表达式"]},{"title":"linux基本操作学习","url":"/2023/08/21/linux_linux%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E5%AD%A6%E4%B9%A0/","content":"目录\n[toc]\n前言及参考资料\n这篇文章主要给出各个指令的作用，详细用法请查阅相关资料，灵活使用man指令和--help选项。仅当我认为重要的时候，会给出详细用法和示例。\n这篇文档会跟随着作者对linux的使用而持续更新。主要是记录一些注意的地方。\n参考资料：\n\nLinux命令行与shell脚本编程大全\n菜鸟教程-Linux指令大全\nlinux的使用\nman的使用\n\nman的使用\n注意man可能会提供9个章节，每个章节有相应内容，请输入man man进行了解，如\n- man 1 man\n- man -f ls：可以查看ls指令有什么页面。\n- 章节有：\n- 1 可执行程序或 shell 命令，用法查这个\n- 2 系统调用(内核提供的函数)\n- 3 库调用(程序库中的函数)\n- 4 特殊文件(通常位于 /dev)\n- 5 文件格式和规范，如 /etc/passwd，编写程序看2-5\n- 6 游戏\n- 7 杂项（包括宏包和规范）， 如 man(7)，groff(7), man-pages(7)\n- 8 系统管理命令(通常只针对 root 用户)\n- 9 内核例程 非标准\n基本指令\necho\n文件系统\n文件管理\n文件权限管理\n文件修改\nln\n用于创建软硬链接。不加参数是硬链接，加上-s参数则是软链接。\n文件压缩与解压缩\n压缩文件指令\ngzip\n可以解决后缀为Z,zip,gz等后缀的压缩文件。比较重要的参数：c（压缩数据输出到屏幕），d（解压缩），t（检查错误），v（显示具体信息），-number（压缩等级，number为1最快，压缩比最差，9反之，默认是6）。\n注意其可以被windows下的winRAR和7zip等软件处理。\n\n坑点：注意压缩后会删除原来的文件！所以最好使用gzip -c file &gt; file.gz进行操作\n另外,zcat,zmore,zless可以用来读取被压缩后的纯文本文件\n\nbzip2\n压缩效果比gzip更好，后缀名是bz2，用法与gzip2差不多\n相应的，有bzcat,bzmore和bzless\nxz\n压缩效果比bzip2更好，后缀名是x2，用法与gzip2差不多\n相应的，有xzcat,xzmore和xzless\nzip与unzip\ntar：文件打包\n注意gzip等压缩对目录压缩的方式是对里面的文件分别进行压缩，导致压缩效果较差，而tar可以进行打包。\n\n-zjJ：三种不同压缩方式，依次为gzip，bzip2，xz.\n-c:进行压缩\n-x:进行解压缩\n-t:查询一个压缩包有哪些文件\n-C &lt;dir&gt; ：解压在dir目录中\n-f &lt;file&gt;：需要处理的压缩文件名file\n-v:展示被处理的文件名\n-p:保留文件原有权限（重要）\n--exclude=&lt;file&gt;：不打包file文件，注意/etc*表示不打包/etc开头的文件\n\n\n压缩： tar -jcv -f file.tar.bz2 &lt;要被压缩的一系列文件和目录&gt;\n查询： tar -jtv -f file.tar.bz2\n解压缩：tar -jxv -f file.tar.bz2 -C &lt;解压到的目录&gt;\n解压指定文件：tar -jxv -f file.tar.bz2 &lt;解压的文件(需要先查询压缩包内的文件)&gt;\n\ndpkg：文件解压缩\n光盘文件制作及刻录\nmkisofs,cdrecord\n其他\n关于备份工具，光盘写入工具和dd等指令，详见鸟哥的Linux厨房一书。\n文档编辑\n参考文档\nlinux三剑客\nfind\n作用：递归列出目录下的所有文件。常配合grep使用。详细用法见此，支持按照文件特征如创建时间，修改时间，创建用户，用户组等查找。\nfind /opt/module/datax/job/export -type f -prune -exec sed -i &#x27;s/hadoop151:3306/hadoop101:3306/g&#x27; &#123;&#125; +\nsed\n主要用于编辑相应文件。一行一行处理。支持新增，取代，打印，删除，插入等功能。详细用法见此\nsed &#x27;3d&#x27; xxx.txt \t\t# 删除文件第3行sed &quot;$d&quot; xxx.txt \t\t# 删除文件最后一行sed ’s/^[ ]*//g’ xxx.txt \t# 删除空格sed &#x27;s/test/mytest/g&#x27; xxx.txt \t# 替换：把test替换为mytest。如果没有g标记，则只有每行第一个匹配的test被替换成mytest。sed ‘s/pattern/&amp;\\n/g’ filename \t# 添加新行（向前）sed -i ‘$ a\\插入字符串’ filename \t# 在最后一行插入字符\n选项说明：\n\n-i，-i.bak:直接在源文件上修改，如果有.bak，则源文件备份到后缀名为.bak的文件里面\n's/pattern/result/g'：将符合正则表达式的 pattern替换成 result，注意s前加n,m表示只修改n到m行的内容。\n-n:只打印修改的内容\n3p:打印第三行的内容，配合-n使用更好，2,9p表示打印2到9行的内容。/pattern/p则是表示显示符合条件pattern的行\n2d:删除第二行的内容，$d则是删除最后一行的内容，/pattern/d则是表示删除符合条件pattern的行\n-e选项表示将结果输出到stdout，不修改源文件\n5a # comment表示在第五行后增添 # comment字符串一行\n\nawk\n非常强大而又复杂的文本处理工具，支持打印相应位置文本，按照指令分隔符分割，过滤文本内容输出等作用。下面是一些简单的介绍，更多具体用法见此\n注意，awk是行处理工具，一行一行处理的。\n格式：\nawk &#x27;PATTERN&#123;ACTIONS&#125; filename&#x27;awk -F&#x27;:&#x27; &#x27;&#123;print $1,$2&#125;&#x27; file                      # 指定冒号为分隔符awk -v&#x27;FS=:&#x27; &#x27;&#123;print $1,$2&#125;&#x27; file                   # 指定冒号为分隔符awk &#x27;BEGIN&#123;OFS=FS=&quot;\\t&quot;&#125;&#123;print $0, $1;&#125;&#x27; filename    # 指定分隔符为制表符awk &#x27;&#123;if(NR&gt;=20 &amp;&amp; NR&lt;=30) print $1&#125;&#x27; test.txt      # 输出20行到30行的内容awk &#x27;NR&gt;=20 &amp;&amp; NR&lt;=30&#123;print $1&#125;&#x27; test.txt           # 输出20行到30行的内容awk &#x27;/re/ &#x27; log.txt                                 # 输出包含re的行awk &#x27;$2 ~ /th/ &#123;print $2,$4&#125;&#x27; log.txt               # 输出第二列包含 &quot;th&quot;，并打印第二列与第四列，～表示模式开始，!~表示模式取反（排除在外）\n注意有以下变量：\nFS输入间隔副，OFS输出间隔符，NR行号，从1开始，NF分隔后的字段数量。\n$x表示第x个分隔后的结果，$0则是整行\n这里再提供一个使用脚本的示例，注意脚本文件格式为awk，第一行需要指定 /bin/awk：\n#!/bin/awk -f# here is cal.awk#运行前BEGIN &#123;    math = 0    english = 0    computer = 0     printf &quot;NAME    NO.   MATH  ENGLISH  COMPUTER   TOTAL\\n&quot;    printf &quot;---------------------------------------------\\n&quot;&#125;#运行中&#123;    math+=$3    english+=$4    computer+=$5    printf &quot;%-6s %-6s %4d %8d %8d %8d\\n&quot;, $1, $2, $3,$4,$5, $3+$4+$5&#125;#运行后END &#123;    printf &quot;---------------------------------------------\\n&quot;    printf &quot;  TOTAL:%10d %8d %8d \\n&quot;, math, english, computer    printf &quot;AVERAGE:%10.2f %8.2f %8.2f\\n&quot;, math/NR, english/NR, computer/NR&#125;\nawk -f cal.awk score.txt\n打印乘法表的有趣示例：\nseq 9 | sed &#x27;H;g&#x27; | awk -v RS=&#x27;&#x27; &#x27;&#123;for(i=1;i&lt;=NF;i++)printf(&quot;%dx%d=%d%s&quot;, i, NR, i*NR, i==NR?&quot;\\n&quot;:&quot;\\t&quot;)&#125;&#x27;\n计算文件大小的有趣示例：\nls -l *.txt | awk &#x27;&#123;sum+=$5&#125; END &#123;print sum&#125;&#x27;\nfind\ngrep\n作用是将输出在命令行上的内容进行匹配筛选处理。\n注意 grep使用正则表达式有点坑。要么开 -e选项，要么用 egrep，要么注意 |要转义才能发挥正则匹配的作用，但 ^和 $作为锚点不需要转义。\n-v反向匹配，匹配到的不输出\n进程管理\nkill\n准确地来说，kill是向相应进程发送中断信号。-9是强制停止。相应信号见此。\ntop\nps\n用户管理\nuseradd, groupadd\n网络相关操作\nssh\n存储管理\n基础知识\n块设备文件\n文件系统介绍\n文件系统驱动都在/lib/modules/$(uname -r) /kernal/fs目录下面\next4\n常用指令\n块设备信息查看\n\ndumpe2fs:查看ext*文件系统的信息\nblkid:列出所有block的id\nlsblk:列出所有block的层级关系\ndf:查看某文件所使用的文件系统\ndu:查看某文件所用量\n\n分区管理与格式化\n\nparted &lt;blk_dev&gt; print:列出某个block的基本信息，注意这个工具还可以进行分区操作。man一下。\ngdisk,fdisk:分区工具，前者是GPT格式，后者是MBR格式，使用前使用parted指令进行查询\npartprobe -s:刷新分区表，使得分区更新立即生效\nmkfs.*:文件系统make工具，给新分区格式化使用\n\nagcount是存储群组数量，一般设置为cpu数量\n务必注意所称的“大小”的单位，在有必要的时候请使用man进行查看。\n\nfsck.*:对文件系统进行检查，注意只有有问题的时候才能使用，否则会造成危害\n\n挂载与卸载\n\nmount -t &lt;fs&gt; UUID=\"\" &lt;dir&gt;:将设备UUID为\"\"的设备挂载到dir下面，使用fs文件系统\n\n有iocharset选项，可以用utf8\n注意-o有很多额外选项，请使用man查看\n\nmount -n -o remount,rw /根目录重新挂载，如果陷入只读模式下可以考虑使用。\ndd if=\"/dev/zero\" of=\"/dev/loop0\" bs=1M count=512 创建一个文件为/dev/loop0（注意这可能是个设备），往里面写0进去，一个block1M，一共512个Block\n如果需要使用一个大文件进行挂载操作（而不是目录），可以利用dd进行初始化，然后利用mkfs对大文件进行文件系统的制作，然后用blkid来查询id，然后利用mount的-o loop选项进行挂载。\n如果需要设置启动挂载，注意开机自动挂载的文件配置在/etc/fstab中。如果用的是大文件而不是物理区块，请使用文件路径名标记。编辑后，请务必使用mount -a检查是否有语法错误。\nmkswap和swapon可以用来制作交换分区。注意也可以使用文件来制作。\n\n管道与重定向\n&gt;覆盖，&gt;&gt;重定向，2&gt;表示stderr的数据重定向。&lt;重定向输入，&lt;&lt;约定EOF标志，如&lt;&lt; \"end\"约定end为EOF标志。\n/dev/null：把数据输入到这里面相当于丢掉所有输出数据\n写入同一文件的语法：echo hello &gt; test 2&gt;&amp;1或echo hello &amp;&gt; test：stdout和stderr的数据全部输入到test中\n管道|：会将上一条指令的stdout作为下一条指令的stdin，如果希望stderr能够被下一条指令处理，需要2&gt;&amp;1进行重定向\ntee：将数据同时输出到stdout和某个文件，双向重定向\nxargs\nxargs -0pne [command]\n\n-0：碰到特殊符号时候转换成一般字符\n-p：执行每个指令时进行询问\n-n：取多少个参数执行\n\n环境变量详解\n\n直接修改 $PATH：export PATH=$PATH:/home/uusama/mysql/bin，注意仅在当前bash有效\n修改 ~/.bashrc或 ~/.bash_profile：永久对当前用户生效，即使打开新的bash\n修改 /etc/bashrc或 /etc/profile或 /etc/environment：永久对所有用户生效，即使打开新的bash\n注意需要 source命令或重启使改变生效。\n\n\n疑问：.bashrc和 .bash_profile的区别？加载顺序会有不同，详见此处\n\n脚本编写\n基本语法\n变量\n\nset&amp;unset：设置变量和删除变量\nreadonly：限定只读\n\n基本运算\n\ntest：用于输出一个表达式的结果\nexpr 2 + 2：注意表达式之间要有空格\n[ $a == $b ]：注意条件表达式放方括号，并且有很多空格\n-le：使用这些命令来比较大小，因为&lt;&gt;被用于重定向了\n!&amp;-a&amp;-o&amp;&amp;&amp;&amp;||：布尔运算\n-z&amp;-n：字符串运算符，后面跟字符串，检查长度是否为0\n$a：检查字符串a是否为空，不为空则为true\n还有文件测试运算符\n\n变量测试与赋值\n\nv=$&#123;var-expr&#125;:var未设置则将v赋值为expr，否则为var\nv=$&#123;var+expr&#125;:var设置则将v赋值为expr，否则为空串\nv=$&#123;var=expr&#125;:var未设置则将var和v同时赋值为expr，否则v赋值为var\nv=$&#123;var?expr&#125;:var未设置则expr输出到stderr，否则v赋值为var\n\n注意这些符号之前可以加冒号，此时var为空字符串也会视为没有设置相应的var变量。\n字符串\n字符串种类\n\n单引号字符串：原样输出，但注意awk的 '&#123;print $1&#125;'\n双引号字符串：支持变量和转义符\n反引号：先执行\n\n字符串变量\ns=&quot;str&quot;echo $&#123;#s&#125;\t# 输出s的长度echo $&#123;s:1:2&#125;\t# 输出trecho `expr index &quot;$string&quot; io`\t# 输出字符rt的下标，这里是1\n字符串匹配\n$&#123;var#key&#125;:从前往后，var中匹配上key的最短部分，去除$&#123;var##key&#125;：从前往后，var中匹配上key的最长部分，去除$&#123;var%key&#125;：从后往前，var中匹配上key的最短部分，去除$&#123;var%%key&#125;：从后往前，var中匹配上key的最长部分，去除$&#123;var/old_str/new_str&#125;：第一个匹配上old_str的部分用new_str替换$&#123;var//old_str/new_str&#125;：所有匹配上old_str的用new_str替换\n例：echo $&#123;PATH##/*:&#125;\n数组\n\n注意数组分隔估计和环境变量 IFS有关，IFS是分隔符，默认是 \\r\\n三个\narray=(1 2 3)array=(123)array[0]=9array[n]=2echo $&#123;array[@]&#125;echo $&#123;array[*]&#125;\t# 两个都是打印整个数组echo $&#123;#array[@]&#125;\t# 打印数组长度# 关联数组（实际就是个map)declare -A sitesite[&quot;google&quot;]=&quot;www.google.com&quot;site[&quot;runoob&quot;]=&quot;www.runoob.com&quot;site[&quot;taobao&quot;]=&quot;www.taobao.com&quot;echo $&#123;site[*]&#125; $&#123;site[@]&#125; \t# 获取所有值echo $&#123;!site[*]&#125; $&#123;!site[@]&#125;\t# 获取所有键\n\n控制结构\n条件表达\n\n(( $a &gt; $b ))：注意两个圆括号可以用大于符号\n((i=1;i&lt;9;i++))：for循环可以用\n[ cond ]:注意要有括空格，注意用变量的时候要用双引号括起来才行，但test可以直接用，大于号和小于号会当成重定向，要加转移符\n[[ cond ]]:bash自己实现的，迁移性较差\ntest cond：\nfor v in $@：注意用循环结构的时候，要区分 \"$@\" \"$*\" $@ $*的区别\n\n控制结构模板\n# ifif [[ cond ]]; then\texpr1elif [[ cond ]]; then\texpr2fi# forfor((i=1;i&lt;=9;i++)); do\texprdonefor var in a b c; do\texprdonefor var in $@; do\texprdone# casecase $var in1)\texpr1;;2|3|4)\texpr2;;*)\tdefaultexpr;;esac# while\n其他\n特殊变量\n\n$&#123;p&#125;：第 p个参数，echo $0的结果是 echo，当 p小于10时可以不带花括号，否则必须带\n$#：参数个数\n$*和 $@：获取参数，其中前者以字符串的形式获取，后者是加引号，然后在引号中返回每个参数。假设在脚本运行时写了三个参数 1、2、3，则 \" * \" 等价于 \"1 2 3\"（传递了一个参数），而 \"@\" 等价于 \"1\" \"2\" \"3\"（传递了三个参数）。\n$?：上一个shell命令的返回状态码\n$$：当前shell环境的进程ID号\n\necho &quot;-- \\$* 演示 ---&quot;for i in &quot;$*&quot;; do    echo $idoneecho &quot;-- \\$@ 演示 ---&quot;for i in &quot;$@&quot;; do    echo $idone# $ chmod +x test.sh # $ ./test.sh 1 2 3# -- $* 演示 ---# 1 2 3# -- $@ 演示 ---# 1# 2# 3\n间隔符\n基本符号的用法\n引号\n\n单引号：将其中的内容全部作为字符串，忽略所有的命令和特殊字符\n双引号：解析特殊字符，包括 ', \", $, \\，如果需要输出这些字符，需要转义符\n反引号：包含一个命令字符串的，其中的命令会先执行，如`ls`等效于 $(ls)\n\n括号\n参考资料见此。\n\n大括号：\n中括号：\n小括号：注意小括号实际上开了一个子shell！\n\n小练习\n练习1：请将git status中，将所有模式处于xxx的文件，全部更改为yyy？\n练习2：将所有文件名后缀为xxx中的文件里，前缀有yyy的单词，统一替换为zzz\n练习3：找出大小超过500MB的所有文件，并将其在git记录中进行--uncached处理（作者遇到的问题，暂时不知道怎么处理？）\n","categories":["linux"],"tags":["基本操作"]},{"title":"git使用场景常见问题解决","url":"/2025/08/06/git_scene/","content":"git stash pop 到错误的分支\n使用场景：你有两个分支main和dev，你在dev的修改临时使用了git stash暂存，之后切换到了main分支，然后使用了git stash pop，之后提示要merge。\n解决方案：首先使用git reset --hard HEAD取消git stash pop对暂存区的修改。这里需要注意的是，如果git stash pop对暂存区造成了conflict需要merge的时候，stash的记录是不会真正弹出来的。因此，之后直接git checkout dev之后，使用git stash pop即可安全恢复。\ngit commit 之后发现不小心加入了不该加入的文件\n如果没有提交到远程仓库：\ngit reset --soft HEAD~1     # 回退到上一个版本，保留工作区和暂存区的代码git reset HEAD your/unwanted/file.txt     # 取消暂存区的文件git rm --cached your/unwanted/file.txt    # 取消追踪文件的更改记录\n之后，记得修改.gitignore文件。\n这里详细解释一下git reset和git rm --cached的区别：前者只是把修改从暂存区移除，但是git仍然在跟踪对应文件；后者表示把修改从暂存区移除，并且git不再跟踪指定文件的修改。之后，在.gitignore里面加入指定文件，之后加入git add .的时候，也不会误加入。\n\n如何表明 git 在跟踪？使用 git commit -m \"message\"，可以看到 git 会提示哪些文件做了修改。你用 git add targetfile.txt之后，就会把对应文件从工作区加入暂存区。但是你如果使用了git rm --cached targetfile.txt之后，即使对应文件有修改，commit的时候也不会提示这个文件做了修改。\n\n如果提交到了远程仓库：\ngit revert HEAD             # 撤销git push\n这里介绍一下git revert &lt;ver&gt;：\n\n如果工作区是干净的，那么撤销操作会根据你在HEAD产生的操作，生成一个逆向操作，同时作用于工作区和暂存区，之后会要求你写一个提交信息，之后会自动提交撤销操作。\n如果工作区有新的修改，会要求你提交你在工作区新增的修改。之后使用revert的时候，会根据你在&lt;ver&gt;的修改，生成一个反向操作。但如果你在后续的提交过程中，和生成的反向操作产生冲突，则需要你手动解决冲突，之后使用git add confilct_file，然后执行git revert --continue。如果想撤销revert操作，则可以使用git revert --abort.\n如果需要撤销范围内操作，需要执行：git revert --no-commit a1b2c3d^..HEAD，最后再commit一下。提交之前，可以利用git status查看一下状态。\n\ngit 仓库下面有其他的 git 仓库，并且其他的 git 仓库是从其他平台(如github)上 clone 下来的\nTODO\n希望从 git 的所有历史记录中抹除密钥的存在\n因为需要重写整个历史记录，因此这是个十分麻烦且危险的操作。在执行之前，需要通知所有协作者。需要使用git-filter-repo。在执行之前，需要备份整个仓库。\n下面这些指令用于抹除敏感字符串。\n# 推荐使用 Python3python3 -m pip install --user git-filter-repo# 为你的项目创建一个裸克隆git clone --bare https://github.com/your-org/your-repo.git# 进入这个新创建的裸仓库目录cd your-repo.gitcat expressions.txt### expressions.txt里面的内容### AKIAIOSFODNN7EXAMPLE### my-super-secret-password### BEGIN RSA PRIVATE KEY# --replace-text 后面跟着你的敏感信息文件git-filter-repo --replace-text expressions.txt  # 会遍历所有版本，将找到的字符串替换为***removed***# --force 参数会用你本地的历史覆盖远程历史git push origin --force --allgit push origin --force --tags\n下面这些指令用于抹除某个文件：\n# 为你的项目创建一个裸克隆（只包含 Git 数据，没有工作区）git clone --bare https://github.com/your-org/your-repo.git# 进入这个新创建的裸仓库目录cd your-repo.git# 将 &#x27;path/to/your/unwanted-file.txt&#x27; 替换成你要删除的文件的真实路径git-filter-repo --path path/to/your/unwanted-file.txt --invert-paths# --force 参数会用你本地的历史覆盖远程历史git push origin --force --allgit push origin --force --tags\n或者使用filter-branch命令：\n# 这个命令会遍历所有分支和标签，从索引中移除指定文件# --ignore-unmatch 确保在没有该文件的提交中不会报错# --prune-empty 会移除因删除文件而变为空的提交git filter-branch --force --index-filter \\  &quot;git rm --cached --ignore-unmatch &#x27;path/to/your/unwanted-file.txt&#x27;&quot; \\  --prune-empty --tag-name-filter cat -- --all# 清理 Git 留下的原始引用的备份git for-each-ref --format=&#x27;delete %(refname)&#x27; refs/original | git update-ref --stdin# 执行垃圾回收，彻底删除悬空的对象git reflog expire --expire=now --allgit gc --prune=now --aggressive# 最后，强制推送到远程git push origin --force --allgit push origin --force --tags\n","categories":["工具链"],"tags":["git"]},{"title":"美区苹果ID注册教程","url":"/2024/05/23/misc_%E7%BE%8E%E5%8C%BA%E8%8B%B9%E6%9E%9CID%E6%B3%A8%E5%86%8C%E6%95%99%E7%A8%8B/","content":"准备\n无\n步骤\n\n登陆https://appleid.apple.com/，选择【创建你的 Apple ID】\n\n注意：邮箱可以使用163以及gmail邮箱，地区选美国，号码可以使用中国大陆号码\n之后进行邮箱何中国大陆号码双重验证即可\n\n然后在你的iOS主机上登录刚刚注册的账号，打开App Store，下载任意一款软件，会要求你验证，同意，然后点击下一步，填写相应信息\n\n注意点：\n\n**一定不要选择任何付款方式，即PayPal和Credit Card\n注意要选择免税州，我选择的是蒙太拿州(Montana)，邮政编号为59043\n其他信息下面这个网站生成，需要注意的是电话号码的填写，此时不是中国大陆电话号码\n\nhttps://www.dizhishengcheng.com/\n\n然后就完成了所有准备步骤了\n\n\n最后，记得用美区账号下载一个梯子\n\n参考教程\n\nhttps://www.zhihu.com/question/19727065/answer/2107953708\nhttps://mp.weixin.qq.com/s?__biz=MzU2NTkwMjIwNw==&amp;mid=2247483734&amp;idx=1&amp;sn=5299b5721367a125194fe6653c579e7c&amp;chksm=fcb5ef46cbc2665086926f707f545ed753b28bcb3365d5380b51169fea6060344891db8519cd&amp;token=569394499&amp;lang=zh_CN#rd\n\n","categories":["misc"],"tags":["Apple ID"]},{"title":"【花里胡哨】更好看的终端配置 - zsh 和 fish","url":"/2025/05/29/misc_%E6%9B%B4%E5%A5%BD%E7%9C%8B%E7%9A%84%E7%BB%88%E7%AB%AF%E9%85%8D%E7%BD%AE/","content":"前言\nzsh 的 theme 很多花里胡哨的还挺好看，可以上 oh my zsh theme wiki 上面查看，但是奇怪的是每次输入命令之后还有点卡，不知道为什么，于是我配置了fish。fish的优点主要是终端可以配置的更花里胡哨，以及可以根据历史记录自动补全命令，这个比Ctrl + R 用的舒服的多，但缺点也不是没有，比如 fish 下面写脚本不是那么习惯，可能和 bash 有点区别。\n配置\nfish 主要通过修改文件~/.config/fish/config.fish 配置。作用类似于~/.bashrc，但是里面编写指令不太舒服，但是可以配置更花哨的终端。下面提供两个示例，复制到上述文件里面就可以用了。先上效果图：\n\n效果大致是，提示符号有七个箭头，会呈现出渐变的效果。如果上一条指令返回值是 -1，则会全红。\nif status is-interactive# i8    # Commands to run in interactive sessions can go hereend# ~/.config/fish/config.fish# ~/.config/fish/functions/fish_prompt.fishfunction fish_prompt    set -l last_status $status    # --- 在 if/else 块之前声明颜色变量，使其作用域为整个函数 ---    set -l user_host_bg_color    set -l path_bg_color    set -l git_branch_color_name    # --- 根据上一条命令的执行状态定义背景颜色和Git分支文字颜色名称 ---    if test $last_status -ne 0        # 命令失败时的颜色        set user_host_bg_color &quot;red&quot;        set path_bg_color &quot;red&quot;        set git_branch_color_name &quot;red&quot;    else        # 命令成功时的颜色        set user_host_bg_color &quot;blue&quot;        set path_bg_color &quot;green&quot;        set git_branch_color_name &quot;yellow&quot;    end    # --- 第一部分：信息行 (这部分保持不变) ---    set_color white --bold --background=$user_host_bg_color    printf &#x27;%s@%s&#x27; (whoami) (prompt_hostname)    set_color normal    printf &#x27; &#x27;    set_color white --bold --background=$path_bg_color    printf &#x27;%s&#x27; $PWD    set_color normal    printf &#x27; &#x27;    set -l branch (command git symbolic-ref --short HEAD 2&gt;/dev/null; or command git rev-parse --short HEAD 2&gt;/dev/null)    if test -n &quot;$branch&quot;        set_color $git_branch_color_name --bold        printf &#x27;(%s)&#x27; &quot;$branch&quot;        set_color normal        printf &#x27; &#x27;    end    # --- 第二部分：7个箭头的提示符 ---    printf &#x27; \\n&#x27; # 行首空格和换行，让箭头在新一行开始    if test $last_status -eq 0        # 命令成功: 7个箭头，首尾随机颜色，中间插值        # -- 辅助函数定义 (仅在此成功分支内有效和定义) --        function __fish_prompt_hex_to_dec_comp            printf &#x27;%d&#x27; &quot;0x$argv[1]&quot;        end        function __fish_prompt_dec_to_hex_comp            printf &#x27;%02X&#x27; &quot;$argv[1]&quot;        end        # -- 辅助函数定义结束 --        set -l palette FF0000 FFA500 FFFF00 00FF00 0066FF 4B0082 EE82EE        set -l num_palette_colors (count $palette)        set -l num_arrows 7        set -l index1 (random 1 $num_palette_colors)        set -l index2 (random 1 $num_palette_colors)        while test $index1 -eq $index2            set index2 (random 1 $num_palette_colors)        end        set -l start_color_hex $palette[$index1]        set -l end_color_hex $palette[$index2]        set -l r_start (__fish_prompt_hex_to_dec_comp (string sub -s 1 -l 2 $start_color_hex))        set -l g_start (__fish_prompt_hex_to_dec_comp (string sub -s 3 -l 2 $start_color_hex))        set -l b_start (__fish_prompt_hex_to_dec_comp (string sub -s 5 -l 2 $start_color_hex))        set -l r_end (__fish_prompt_hex_to_dec_comp (string sub -s 1 -l 2 $end_color_hex))        set -l g_end (__fish_prompt_hex_to_dec_comp (string sub -s 3 -l 2 $end_color_hex))        set -l b_end (__fish_prompt_hex_to_dec_comp (string sub -s 5 -l 2 $end_color_hex))        # *** CORRECTION IS HERE ***        for i in (seq 1 $num_arrows) # 使用 (seq 1 $num_arrows) 代替 from ... to ...            set -l current_arrow_color_hex            if test $i -eq 1                set current_arrow_color_hex $start_color_hex            else if test $i -eq $num_arrows                set current_arrow_color_hex $end_color_hex            else                set -l t (math --scale=10 &quot;($i - 1) / ($num_arrows - 1)&quot;)                set -l r_interp (math &quot;round((1 - $t) * $r_start + $t * $r_end)&quot;)                set -l g_interp (math &quot;round((1 - $t) * $g_start + $t * $g_end)&quot;)                set -l b_interp (math &quot;round((1 - $t) * $b_start + $t * $b_end)&quot;)                set r_curr (math &quot;max(0, min(255, $r_interp))&quot;)                set g_curr (math &quot;max(0, min(255, $g_interp))&quot;)                set b_curr (math &quot;max(0, min(255, $b_interp))&quot;)                set current_arrow_color_hex (__fish_prompt_dec_to_hex_comp $r_curr)(__fish_prompt_dec_to_hex_comp $g_curr)(__fish_prompt_dec_to_hex_comp $b_curr)            end            printf &#x27;%s➤ &#x27; (set_color $current_arrow_color_hex)        end        set_color normal    else        # 命令失败: 7个红色箭头 (这里的循环语法 &quot;for i in (seq 7)&quot; 是正确的)        set_color red --bold        for i in (seq 7) # (seq 7) 会生成 1 2 3 4 5 6 7            printf &#x27;➤ &#x27;        end        set_color normal    end        printf &#x27; &#x27;end\n然后下面这个则是七个箭头呈现彩虹色的效果，比较一般，图标就不展示了，就放在这里吧。\nif status is-interactivei8    # Commands to run in interactive sessions can go hereend# ~/.config/fish/config.fish# ~/.config/fish/functions/fish_prompt.fishfunction fish_prompt    set -l last_status $status    # --- 在 if/else 块之前声明颜色变量，使其作用域为整个函数 ---    set -l user_host_bg_color    set -l path_bg_color    set -l git_branch_color_name    # --- 根据上一条命令的执行状态定义背景颜色和Git分支文字颜色名称 ---    if test $last_status -ne 0        # 命令失败时的颜色        set user_host_bg_color &quot;red&quot;        set path_bg_color &quot;red&quot;        set git_branch_color_name &quot;red&quot;    else        # 命令成功时的颜色        set user_host_bg_color &quot;blue&quot;        set path_bg_color &quot;green&quot;        set git_branch_color_name &quot;yellow&quot;    end    # --- 第一部分：信息行 ---    # 1. 用户名@主机名    set_color white --bold --background=$user_host_bg_color    printf &#x27;%s@%s&#x27; (whoami) (prompt_hostname)    set_color normal    printf &#x27; &#x27;    # 2. 完整的绝对路径    set_color white --bold --background=$path_bg_color    printf &#x27;%s&#x27; $PWD    set_color normal    printf &#x27; &#x27;    # 3. Git 分支信息    set -l branch (command git symbolic-ref --short HEAD 2&gt;/dev/null; or command git rev-parse --short HEAD 2&gt;/dev/null)    if test -n &quot;$branch&quot;        set_color $git_branch_color_name --bold        printf &#x27;(%s)&#x27; &quot;$branch&quot;        set_color normal        printf &#x27; &#x27;    end    # --- 第二部分：7个箭头的提示符 ---    printf &#x27; \\n&#x27; # 行首空格和换行，让箭头在新一行开始    if test $last_status -eq 0        # 命令成功: 7个彩虹色箭头        set -l rainbow_colors FF0000 FFA500 FFFF00 00FF00 0066FF 4B0082 EE82EE # 红 橙 黄 绿 蓝 靛 紫        for color_hex in $rainbow_colors            printf &#x27;%s➤ &#x27; (set_color $color_hex)        end        set_color normal    else        # 命令失败: 7个红色箭头        set_color red --bold        for i in (seq 7)            printf &#x27;➤ &#x27;        end        set_color normal    endend\n后记\n因为作者有一堆需要加载的环境变量放在~/.bashrc里面的，但是迁移到~/.config/fish/config.fish会比较麻烦，fish的脚本有一套固定的语法，所以作者是这样解决的：在~/.bashrc的最后一行启动fish，这样环境变量也就无缝继承过去了。然后试了一下，在 fish 里面执行脚本应该也是没有问题的有问题，似乎就bashrc类似的初始化配置的迁移比较麻烦？就这样吧，渐变色的箭头我还是比较喜欢的。\n比如没有 unset指令，可以在前文提到的文件最后面加上alias unset 'set -e'解决。应该主要是一些内建指令可能有区别，不知道脚本语法有没有区别，应该是有的。比如上面的函数，if的结尾是end而不是fi.\n","categories":["Linux"],"tags":["fish","bash","折腾","鼓捣"]},{"title":"【疑难杂症】记录从源码构建项目时碰到的一些坑","url":"/2024/01/11/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E8%B8%A9%E5%9D%91/","content":"CMake构建环境\n关于CMakeCache.txt引发的一些坑\n作者在配置环境时，因为某些原因删除了某些依赖，但是CMake仍然检测到依赖的存在。原因是CMakeCache保存了一些记录，下次重新CMake的时候会从这里获取内容。因此，如果改变了依赖，务必删除CMakeCache.txt。\nconfigure构建环境\n","categories":["疑难杂症"],"tags":["环境配置"]},{"title":"ChatGPT4开通教程","url":"/2024/05/23/misc_ChatGPT4%E5%BC%80%E9%80%9A%E6%95%99%E7%A8%8B/","content":"前置条件\n美区账号，以及美国机场\nChatGPT4开通步骤\n\n下载支付宝\n\n支付宝定位改到旧金山\n搜索出境\n找到【折扣礼卡】\n\n找到App Store &amp; iTunes US\n\n接着会跳转到Pockyt Shop进行简单的注册，最后购买即可\n\n返回App Store\n\n搜索ChatGPT并下载\nChatGPT登录自己的账号，注意要挂美国节点\n进入设置，找到【升级到Plus版本】之类的字眼，升级即可\n\n完成！\n\n参考教程\n\nhttps://zhuanlan.zhihu.com/p/686235951\n\n","categories":["misc"],"tags":["ChatGPT4"]},{"title":"【课程复习】计算机系统结构复习","url":"/2023/07/29/%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0_%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84%E5%A4%8D%E4%B9%A0/","content":"前言\n这篇文章是博主复习用的资料，对所教授内容不熟的几乎没有参考价值。如果有需要，可以联系博主，在相应板块详细解释（如果有时间的话）\n引言及历史\n计算机系统结构和计算机组成的区别：\n- 计算机组成：描述对用户透明的结构实现。流水线结构，Cache都是其中的研究对象。\n- 计算机结构：从用户的角度描述计算机。指令集，可见的寄存器，内存管理表的结构都是其中的研究对象。\n指令集架构关注的问题：寄存器，内存寻址，寻址模式，指令操作，可用操作，控制流指令，指令编码。（指令集 + 架构）\n摩尔定律：每18个月，处理器的性能翻番\n并行级别：\n- 应用层面：数据级并行、任务级并行\n- 架构层面：指令级并行（单个周期执行多条指令）、线程级并行、请求级并行、向量架构（GPU）\n并行分类：单指令单数据（SISD），单指令多数据（SIMD），如向量架构，GPU，多指令单数据（MISD），多指令多数据（紧耦合MIMD，松耦合MIMD）\nISA：RISC（RISC-V，MIPS，ARM），CISC（x86）\nNewmann架构：主存，逻辑计算单元，程序控制单元，IO控制单元\n程序：一系列顺序指令\n量化研究方法\n\n动态功耗：晶体管极性转换产生的功耗，\\(P_{动态功耗}=\\frac{1}{2}CV^2f\\)，其中f是翻转频率\n静态功耗：通电不工作产生的功耗，\\(P_{静态功耗}=UI\\)\n\n性能定义：\n- 执行任务所需时间：执行时间，响应时间，已用时间，时延\n- 单位时间执行任务：执行率，带宽，吞吐量\n计算机设计原理：局部性原理，加速常见时间，阿姆达尔原理（加速某一部分带来的增益是有限的）\nCPU Time = CCT * IC * CPI\n\n影响因素：\nCCT（Clock Cycle Time）：硬件组织（流水线设计）和硬件技术\nIC（instruction count）：Program，Compiler，ISA（只要跟指令有关，就会影响指令数量）\nCPI (clock cycles per instruction)：ISA（ISA涉及到流水线架构，所以会影响）和硬件组织\n\n指令\nISA(Instruction Set Architecture)包括：编程寄存器、操作数访问、操作数的类型和大小、指令集、寻址模式、指令编码（是软件和硬件之间的接口，包不包括流水线架构呢？不包括吧。。。不太理解，但是首先肯定的是那一套的Inst是包括的，然后操作数相关，寻址相关也是配套的）\n作用：将指令集规范和硬件实现解耦开，高级编程语言和抽象机器解耦开。\nISA的分类：\n\n栈式架构：有一个隐式操作数，在栈顶部\nAC（Accumulator）架构：有一个隐式操作数，是累加器\n通用寄存器计算机体系结构：只有显式操作数，要么在寄存器要么在内存。可以细分为操作数全来自于寄存器(MIPS)，或者一个来自于寄存器一个来自于内存的架构(80x86)，或者全来自于内存的架构（VAX，已经绝种了）\n\n（思考以上五个架构的优缺点？栈是放在内存的，Accumulator免不了前后数据的强相关使得流水线困难）\n对于一条指令来说，最多可含有的内存地址数和最多的操作数也是值得关注的。如x86支持一条内存地址，但MIPS不行。\nISA设计应当关注的问题：寻址模式，操作数种类和大小，指令种类，指令控制流，指令集编码\n寻址模式：\n\n立即数寻址：operand = imm\n直接寻址：operand = Mem[imm]\n间接寻址：operand = Mem[Mem[imm]]\n寄存器寻址：operand = R[imm]\n寄存器间接寻址：operand = Mem[R[imm]]\n相对寻址：operand = PC + imm\n堆栈寻址：取栈顶操作数\n\n指令类型：算术&amp;逻辑、浮点数，数据传输、控制、系统\nMIPS所拥有的寄存器：32个GPRs，32个浮点数寄存器，PC，HI，LO，AT，GP，SP，FP，RA\n大小端问题：大端是数字高位存储在地址小的地方，小端是数字低位存储在地址小的地方。（如何理解记忆？寻址都是由小到大寻址，那么大端是数字高位（大）放在地址小的地方，按顺序来）\n流水线\n流水线实际要考虑的问题：\n\n流水线每个阶段的开销\n流水线寄存器的延迟（寄存器也是有延迟的，见CSAPP，因为这个原因不可能实现无限加速）\n时钟歪斜（同一时钟信号到达不同部件的时间不完全相同）\n\n流水线冒险：\n\n结构冒险：所需资源繁忙（如RAM在多个阶段被需要，如I-RAM和D-RAM结合在一起的这种），以及超标量流水线，一个阶段有多条指令要写寄存器堆，但寄存器堆只有一个写端口\n数据冒险：RAW，WAR，WAW，五级流水线只有RAW，但超标量都会碰到。关注提交阶段（尤其是从内存读数据）相关的冒险，因为这个时候往往会出现不可前推的冒险，需要停顿。\n控制冒险：因为控制流的改变无法提前得知，部分紧跟控制指令后的指令将会被取进流水线执行（但不应执行），需要消除这些指令产生的影响。\n\n异常处理：\n\n异常（Exception）：程序执行期间异常引发的内部事件，如页面错误，下溢，除零等。异常处理在五级流水线同一在提交阶段处理。\n中断（Interrupt）：程序执行期间的外部事件，如IO中断，通常是异步的形式出现\n陷阱（Trap）：由于异常或中断导致控制权移交给管理程序，通常是同步的形式出现\n\n分支预测\n分类：\n\n静态分支预测：总是预测跳转或不跳转\n动态分支预测：根据历史跳转记录判断跳不跳转\n\n2-bit Predictor：用2bit来记录分支历史状况。可以看成FSM。缺点：在某些特定程序会一直预测不成功。（在FSM的两个状态之间来回抖动）\n分支历史记录表（Branch History Table, BHT）：取PC的低几位作为表的Index，每个表项为2-bit Predictor。\n局部动态分支预测和全局动态分支预测详见《超标量处理器设计》\nCache设计\n存储器基础\n随机访问：访问每个位置都是相同时间。DRAM，SRAM，磁盘。底层电路不太了解，略。\n局部性原理\n空间局部性和时间局部性，例子是矩阵乘法（考虑矩阵在内存里的组织形式，考虑缺失率，kij或ikj的迭代方式才是最快的）\nCache组织\nCache与内存的相连方式\n需要讨论的问题：块大小，地址编码格式？addr=tag##index##offset.\n相连方式：一一映射，组相联，全相连。组相联是有\\(2^{index}\\)个组，然后每组有个tag进行比对，比对成功则对应，否则不行。全相连是特殊情况，addr=tag##offset。\n归纳一下，一一映射是多个组，一个路，组相联是多组多路，全相联是一组多路。也就是，组是分组个数，路是一组有多少个块。\nCache的读写策略\n需要考虑的问题：读or写？命中or缺失？缺失时的块驱逐策略？\n块驱逐策略：Random，LRU，PLRU，FIFO\n&gt; 写透：写命中时，数据同时写入Cache和内存。（读缺失不会产生写操作）\n&gt; 写回：写命中时，只写入Cache。当该块被替换时检查dirty位，如果dirty位为1则写回内存，否则不写。（读缺失会产生写操作）\n&gt; 写分配：写缺失时，会发生块驱逐，然后读出相应块到Cache，然后重新写。（比较复杂）\n&gt; 写不分配：写缺失时，直接将结果写入内存。\n写缓冲：与Cache并行放置，目的是为了让流水线不停顿（尤其是写透）。当缓冲区也爆掉的时候，会停顿CPU。\n一般是写回搭配写分配，写透搭配写不分配。原因？\n一般L1和L2 cache采用xxx，L3采用xxx。原因？\nCache性能评价\n平均访存时间（Average Memory Access Time，AMAT），计算和访存以及命中率有关。注意访存包括指令访存以及数据访存。\n\\(AMAT(L_i) = Hit Time + Miss Rate \\times Miss Penalty = Hit Time + Miss Rate \\times AMAT(L_{i-1})\\)\n\n降低缺失率：扩大block size，扩大cache size，更高的相连度\n降低缺失惩罚：多级cache，读优先\n降低命中时间：避免在索引cache时的地址转换（如虚拟地址到物理地址的转换）\n\n多级cache的任务：L1的速度尽量跟cpu对齐，L2尽量够大，防止访问主存。分类有：\n&gt; 多级包含：L1的数据全部在L2中，有利于数据的一致性，但因为L1的数据全部在L2中，故L1的缺失率会略高\n&gt; 多级排除：L1的数据不一定全部在L2中，优点是防止空间浪费\n指令级并行\n编译器技术\n\n包含议题：循环展开，软件流水线，超标量处理器\n基本块：一个顺序指令序列，单入口单出口\n基本块内的指令级并行很小，内部很容易出现数据冒险，需要考虑循环级并行（LLP）\n数据依赖：分为真数据依赖和名称依赖（反依赖，可能导致WAR和输出依赖，可能导致WAW）\n名称依赖：两条指令引用相同寄存器或内存位置。WAR中出现的依赖往往被称为反依赖。（注意依赖不一定会引发数据冒险，但数据冒险一定是由于数据依赖引发的）。而WAW中出现的依赖往往称为输出依赖。利用寄存器重命名技术可以解决名称依赖。\n控制依赖：某部分代码块对某些条件产生的依赖。\n可以提高ILP的编译器优化技术：循环展开，软件流水线，基于编译器的分支预测\n对于编译器来说，如果发现寄存器相关，可以考虑指令重排，将发生数据冒险的指令，在不改变功能的前提下分隔得尽量远。\n\n循环展开步骤：\n①重复：将循环体的内容重复几次\n②合并：在逻辑上将某些指令的常量参数予以修改，并合并某些不必要的操作（等效转换）\n③寄存器重命名：消除寄存器前后依赖，目的是提高并行性\n④调度：进行指令重排，目的是减少流水线停顿\n超标量处理器的分类：\n&gt; 静态调度超标量处理器：由编译器实现\n&gt; 动态调度超标量处理器：由特殊硬件实现，使用计分板算法或Tomasulo算法\n超长指令字（VLIW）处理器：VLIW并行执行多条指令（一条指令完成多个操作）。VLIW架构中，指令级并行的发现与指令执行顺序的调度（硬件中最困难的部分）完全交由编译器完成。\n软件流水线：对于每次循环，内部的循环如果是独立的，那么可以通过重排不同循环的指令使不同循环的指令交错，获得ILP。\n软件流水线与循环展开的比较：\n&gt; 可以一起使用\n&gt; 软件流水线占用更少的空间\n&gt; 分析复杂，寄存器管理很复杂\n动态调度算法\n动机：乱序执行，利用重命名技术解决依赖关系\nScoreboard没有重命名，但Tomasulo有。\nScoreBoard\n策略：在一系列指令窗口内动态构建依赖关系图。\n乱序执行将解码阶段细分为两个阶段：\n\n发射阶段：负责解码指令，检查结构冒险\n读操作数阶段：在没有数据冒险的状况下读操作数\n\n顺序发射，乱序执行，乱序提交。\n算法经历的四个阶段：\n\n①发射：检查是否有WAW冒险，结构冒险，如果有则发射停顿，否则进行解码操作\n②读操作数：没有数据冒险（RAW）后进行读操作\n③执行阶段\n④写回阶段：该阶段检查WAR（写回的时候，之前的指令还没有读，因为是乱序发射），如果有的话则停止直到没有为止。\n留意三种数据冒险在何时处理。\n\n算法相关步骤：\n\n①指令状态：发射（处理WAW），读操作数（处理RAW），执行和写回（处理WAR）。\n②功能单元状态：\n- 功能单元：指的是加减乘除那些机构。\n- 使用九元组来表示功能单元：\n- busy：该功能单元是否忙\n- op：该功能单元执行的操作\n- Fi：该功能单元的目的寄存器\n- Fj, Fk: 该功能单元的源操作数寄存器\n- Qj, Qk（Query）: 产生Fj，Fk操作数的功能单元\n- Rj, Rk（Ready）: Fj，Fk操作数是否产生，产生时为1，产生后进行读，读后将该两位标志置0.\n③寄存器结果状态（Result）：指示哪个功能单元要写哪个寄存器（是个映射机构），注意记录的是正在计算的，计算完了的则无需记录。\n\n\n\n\\(Issue\\)：Wait Until的意思是所需功能单元空闲并且当前指令的目的寄存器不能是正在执行的某条指令的目的寄存器（保证不会出现WAW），而Bookkeeping的Rj和Rk的限制保证不会出现RAW。\n\\(Read Operands\\)：需要两个操作数Ready了才能读，读后记得清空。\n\\(Write Result\\)：Wait Until的意思是任何一个功能单元的两个源寄存器（已计算好，因为可能是当前功能单元在算的）不能成为当前单元的目标寄存器（应对WAR冒险，保证WAR的顺序）。这个阶段更新所有其他指令的计分板条目内容，告知操作数已经Ready。并且清除当前指令的Result和Busy位。\n\nPPT上有Scoreboard样例，不再赘述。需要关注的是何时发射（没有WAW），何时读操作数（没有RAW），何时写回（没有WAR），何时更新计分板（发射时更新基本信息，读操作数时更新R，写回时更新计分板上所有的Q，Result和Busy）。\nScoreboard算法的约束条件：可以并行执行的指令条数（即不会发生数据冒险的指令条数最多数目），计分板数据结构的大小（计分板的大小直接决定最多可并行执行的指令条目数，决定了指令窗口的大小），功能单元的数目，指令种存在依赖关系的指令数目多少。\n注意Scoreboard仍然是顺序发射不是乱序发射，但是是乱序执行和提交。\n关于硬件上的编写细节：组合逻辑？时序逻辑？想清楚。从过程上看，是时序逻辑不是组合逻辑。\nTomasulo\n动机：提高浮点数单元的性能\n主要特征：拥有一个保留站（reservation stations），通过寄存器重命名技术解决WAR和WAW问题，跟踪每个操作数用于解决RAW问题。\n\nTomasulo 保留站条目元素：\n\nOp:进行的操作\nVj,Vk:源操作数的值（注意这点跟Tomasulo不同）\nQj,Qk:产生源操作数的寄存器\nA:保存读写内存的地址\nBusy\n寄存器结果状态：和计分板的一样。\n\n\n\nTomasulo和Scoreboard算法的比较：\n- 结构性冒险更少，基于Common Data Bus的旁路，对于WAR，WAW冒险无需停顿（通过寄存器重命名技术解决）\nTomasulo算法的缺点：无法实现精确中断（因为乱序提交的缘故）\n未看完，待补。\n数据级并行（未讲，待补）\nSIMD\n线程级并行（未讲，待补）\n待补\n","categories":["arch"],"tags":["计算机系统结构","课程复习"]},{"title":"【疑难杂症】记录VsCode使用的疑难杂症","url":"/2024/01/11/%E7%96%91%E9%9A%BE%E6%9D%82%E7%97%87_1_vscode/","content":"Python的左键快速跳转\n我在一个地方安装了一个库，但是为了便于管理，我并未将其放置在/envs/lib/python3.*/site-packages下面，而是其他地方，左键应当如何跳转？编辑extraPaths即可。可以去设置，也可以创建文件：\n&#123;  &quot;python.analysis.extraPaths&quot;: [    &quot;./software/nest-simulator/3.4-gcc-8.4.0&quot;  ]&#125;\nRemote SSH连不上（cmd闪退）\nRemote SSH突然有一天连不上服务器了，但是可以用XShell连接。OUTPUT报错有什么什么async啥啥啥的报错（没记录，复现不了了），于是求助于chatGPT：\n\n你遇到的问题是 Visual Studio Code 的 Remote SSH 扩展突然无法连接，并显示错误信息“Error: Got bad result from install script”。根据从不同的 GitHub 问题讨论中获得的信息，这个问题可能由多种原因引起，解决方案也各不相同。以下是基于用户经验的几种可能的解决方案：\nWindows 注册表问题：一些用户发现问题与他们的 Windows 注册表设置有关。具体来说，修改或清除注册表键 HKEY_LOCAL_MACHINEProcessor 和 HKEY_CURRENT_USERProcessor 中的 AutoRun 值解决了他们的问题。这些键默认情况下不应该存在，或者可能包含需要修复或删除的命令字符串。\n权限和 SSH 访问问题：另一个常见的原因与权限或 SSH 访问差异有关。确保你的 SSH 密钥正确设置，并且你有权限访问远程机器。有时，重新配置或更新 SSH 密钥可以解决问题。\n重新安装 VS Code 或 Remote SSH 扩展：一些用户通过重新安装 Visual Studio Code 或 Remote SSH 扩展解决了问题。如果问题是由于软件安装损坏或版本过旧，这可以帮助解决。\n检查 cmd.exe 功能：在少数情况下，问题与用户本地机器上的 cmd.exe 问题有关。用户发现他们的 cmd.exe 崩溃或功能不正常，这反过来影响了 Remote SSH 扩展。修复 cmd.exe 的问题，如确保它可以正确打开和运行，可能是一个解决方案。\n请记住，具体的解决方案可能会根据你的本地和远程机器的具体配置和环境而有所不同。如果这些步骤不能解决你的问题，你可能需要查看你收到的具体日志和错误信息，以便进行更详细的故障排除。\n有关此问题的更多详细信息和特定用户经验，你可以参考 GitHub 上的讨论：问题 #5693、问题 #5803 和 问题 #5722。\n\n好，最近突然也出现了打开cmd闪退的问题，直接聚焦到最后一点，尝试解决cmd闪退问题。Google一下，改了注册表参数：HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor的AutoRun的参数改为空（即删除），再试一下，成功。\nRemote SSH又连不上了（更新服务器端侧VsCode-Server）\n需要打开本地顶部bar的Help-About，找到VsCode版本对应的SHA值，比如我当前的SHA值是863d2581ecda6849923a2118d93a088b0745d9d6，之后使用ssh -i &lt;密钥路径&gt; &lt;用户名&gt;@&lt;ip地址&gt;登录服务器，进入~/.vscode-server/bin/&lt;SHA&gt;，之后在里面使用wget -c https://update.code.visualstudio.com/commit:&lt;SHA&gt;/server-linux-x64/stable（如果服务器不连接外网，则可以考虑本地scp或者使用xftp等工具传输过去），然后在里面解压，保证~/.vscode-server/bin/&lt;SHA&gt;路径下的目录结构是bin，extension...（也就是解压出来的东西直接在这个目录下面），然后重新连接，完成。（注意尖括号需要换成对应的内容）。\nRemote SSH连上了，但是没完全连上\n症状：没有在左侧打开任何目录时候(Open Directory)，可以连上remote server并打开cmd，但是一打开目录，右下角提示奇奇怪怪的错误，但如果打开右下角的弹窗，点开Developer Tools，就可以发现报错：ENOPRO: No file system provider found for resource。没找到原因，但是在这里找到了解决方案：\n&gt; this error only happen (in my case) when \"remote.SSH.useExecServer\" and \"remote.SSH.serverInstallPath\" both true, and \"remote.SSH.defaultExtensions\" not empty.\n也就是说，在设置里面去掉一个勾就可以了，实证有效，但后果未知。\n","categories":["疑难杂症"],"tags":["Vscode"]},{"title":"【计算机系统结构：量化研究方法】第四章：向量、SIMD和GPU结构中的数据级并行","url":"/2023/11/30/%E9%87%8F%E5%8C%96%E7%A0%94%E7%A9%B6%E6%96%B9%E6%B3%95_ch4_%E5%90%91%E9%87%8F%E4%B8%8ESIMD/","content":"前言\nSIMD有三个变体：向量体系结构，多媒体SIMD指令集，图形处理器。本章着重介绍了这三者。在最后，还谈了循环相关性的问题。\n向量体系结构\n向量体系结构的主要操作是：有一组专门的寄存器叫做向量寄存器，一个向量寄存器里面能保存很多元素。一条指令可以对一个向量寄存器的元素施加同一个操作。这就是SIMD的体现。由于向量寄存器的元素很多，因此向量的载入和存储实现了流水化。需要注意的是，载入和存储的时延一般很长。\n对于向量体系结构来说：因为单个向量内产生的相关性而出现的转发操作被称为链接(chaining)。执行单个元素的功能单元被称为车道，向量长度为一个向量的元素大小（而并非字节数量）。护航指令组(convoy)是指一组能够一直执行的向量指令（按照书上的理解，感觉是指可以一起发射的向量指令），需要注意的是，其内部不能出现结构冒险（出现的地方：功能单元个数，寄存器读写端口数量，访存数量），数据冒险通过链接的方式解决。执行一个护航指令组所花费的时间被称为一个钟鸣(chime)，一个钟鸣的大小为一个向量指令执行所需的周期数。钟鸣模型的开销：单周期发射指令数限制，启动时间（流水线越深，这一时间可能越久）提升单条向量指令的运算速度的手段是增加车道数，但相应的，需要考虑功耗的问题。\n接下来，向量体系结构有两个很重要的寄存器：向量长度寄存器(VLR)和向量遮罩寄存器。前者记录需要进行运算的元素个数（因为单个元素都要占据一个车道），但是其中的值不能大于向量本身的元素个数，即最大向量长度(MVL)。而向量遮罩寄存器用来处理代码中的IF语句。如果IF为真，控制位为1，进行某些运算，否则控制位为0，不参与运算。\n由于向量体系结构具有大量元素，因此需要向量载入存储单元提供带宽，于是向量处理器使用存储器组，将大量对象分散到多个存储器中，以提供更大的带宽。需要注意的是，由于需要载入非连续值，因此需要进行独立的组寻址。由于存储器组对地址空间进行了划分，我们需要特别关注某些具有固定步长的访问方式，比如矩阵乘法。如果步幅大小不合适，导致每次访存都是少数几个存储器的话，由于结构冒险，会导致一定程度的性能下降。除此之外还支持按照某个数组存的下标进行访存：A[B[i]]，这种访存操作被称为集中-分散操作，典型的应用就是稀疏矩阵索引。\nSIMD指令集多媒体扩展\n这里首先强调一下SIMD指令集多媒体扩展和向量体系结构的不同：\n\n固定了数据操作数的个数，还有向量长度寄存器，有隐含的最大向量长度，但不提供向量遮罩寄存器\n没有复杂的寻址模式，即集中-分散和步幅寻址\nSIMD指令集多媒体扩展数据传输要求在存储器中对齐，以避免页错误的产生，并且其不需要大量存储器带宽支持\n\nx86提供的扩展：\n\nMMX：8*8位运算或4*16位运算\nSSE：16*8位运算或8*16位运算或4*32位运算，开始支持单精度浮点运算，在SSE2、SSE3、SSE4提供了双精度浮点运算\nAVX：寄存器宽度变为256位\n\nRoofline可视性能模型：\n就是一个浮点运算次数关于运算密度的函数，其中：\n\\[\n 可获得的GFlop/s = \\min(峰值存储器带宽\\times 运算密度，峰值浮点性能)\n\\]\n而运算密度等于浮点运算数与所访问存储器字节的比值。峰值浮点性能表征的是计算能力，受限于硬件，使用硬件规范（？翻译问题？Hardware Spec.）求得；峰值存储器带宽使用Stream基准测试求得。\n图形处理器\n以CUDA为例：CUDA编程模型定义为单指令多线程(SIMT)，于是出现了线程块的概念，一个线程块有16条线程，一条指令处理多个元素（32个元素？）。执行线程块的硬件称为多线程SIMD处理器，而负责指派线程块给处理器的是线程块调度程序（由此可见一个处理器处理一个线程块，因此处理器内部还有线程调度程序）。大量功能单元都是深度流水化的。\n每个SIMD处理器有特定的车道数，对于某些架构可能是16个，那么一个含有32个元素的指令需要两个周期。（也就是说，一个时刻只可能有一个线程在执行，而不是多个线程执行，宏观并行微观串行）。处理器使用计分板管理跟踪线程，多线程可以隐藏访存延迟。\n一个SIMD处理器有32768个32位的寄存器，但每个SIMD线程最多拥有64个向量寄存器，每个向量寄存器有32个元素。\n\nFermi有16个车道，每个车道有2048个32位的寄存器（如果是处理32位元素，那么就有64个元素的位置）。为了能处理SIMD的32个元素，CUDA线程可以使用2048个寄存器的一半。（？这个部分的数量关系实在没弄清楚）\n\nNVIDIA GPU使用PTX(并行线程执行)作为其指令集。该指令集提供了谓词寄存器（有点类似于遮罩寄存器的作用），控制相关指令是否执行，控制流相关的指令有函数的call,return，线程的branch和exit，以及线程块内的屏障同步。CUDA为每个线程块指派特定编号blockIdx，为每个线程同样提供特定编号threadIdx。GPU的所有访存都是集中-分散的，因此有一个特殊的硬件结构：地址接合硬件，负责判断SIMD线程的多个车道何时一同发出顺序地址，将这些地址接合在一起，打包通知存储器进行传输。\n在处理条件分支的时候，PTX使用指令分支，调用，返回和退出描述。具体来说，每个车道都有一位的谓词寄存器，如果谓词寄存器为0，则不执行指令；为1，则执行指令。16个车道组成了宽度为16位的遮罩寄存器，遮罩寄存器的值用栈进行管理。以if-else-end为例：在if操作的时候执行push操作，将旧值压栈，然后遮罩寄存器设置新值；else操作对遮罩寄存器求补；end后弹栈。需要注意的是，实际上每个元素将每个分支的指令在逻辑上都过了一遍，是否执行则取决于遮罩寄存器对应位上的值，这一点跟单核处理器的思路完全不同（毕竟每个车道都要执行相同的指令，SIMD嘛）。此外，还进行了一定优化：遮罩寄存器全为1或全为0，会跳过某些指令。\n对于GPU来说，有三级存储器结构：单个CUDA线程使用的叫专用存储器，一个线程块使用的叫本地存储器(用__shared__关键字表示)，所有线程块共享的叫GPU存储器。\n最后，4.4.7的表4-8和4.4.8的表4-9比较了GPU与前两种SIMD实现方式的不同。这里不具体展开（挖坑，后面也许会展开）。\n循环相关性处理\n由于循环实现向量化的基础是没有循环相关性，所以循环相关性常常受到关注。如果一个循环没有循环相关性，往往采用条带挖掘(strip mining)的技术进行循环向量化：在循环开始之前将不足MVL的部分计算出来，之后在每次循环都计算MVL的量。\n如果一个循环语句构成循环间依赖，但是未出现循环依赖，那么这种循环有机会消除循环间依赖改写成可并行化的循环：\nfor (i = 0; i &lt; 100; i++)&#123;    a[i] = a[i] + b[i];         // S1    b[i + 1] = c[i] + b[i];       // S2&#125;\n注意到S1依赖于上一次循环的S2，但是循环内，S2不依赖S1，那么可以稍作修改：\na[0] = a[0] + b[0]for (i = 0; i &lt; 99; i++)&#123;    b[i + 1] = c[i] + b[i];       // S2    a[i + 1] = a[i + 1] + b[i + 1];         // S1&#125;b[100] = c[99] + d[99]\n这样修改后，每个循环体可以并行执行，只需要保持循环内顺序即可。\n对于循环间的数组索引，我们称其为仿射的，如果其索引符合ax+b的形式，其中x是循环变量。而稀疏矩阵为典型的非仿射索引。如果循环内有两个仿射索引ax+b和cx+d，我们通过检查\\(0\\equiv (d-b) (\\mod gcd(a-c))\\)来判断是否构成循环间相关。如果等式成立，则构成循环间相关。\n实际上循环间相关的检测是一个NPC问题，如果检测到循环间相关，编译器可以采用寄存器重命名和复制操作来消除这种相关。\n参考资料\n计算机系统结构：量化研究方法（第五版）\n","categories":["arch"],"tags":["计算机系统结构","读书笔记"]},{"title":"【bug沉思录】1 - 当 thread_local 碰上 openmp 的嵌套并行","url":"/2025/08/07/cppbug_1_threadlocal/","content":"bug复查流程\n这次碰到的bug十分隐蔽，如果没有做好git管理的话，在历史记录上二分查找bug，那么这个bug估计很难查出来。原因是实现逻辑是正确的，但是因为各个线程使用的种子是完全相同的，导致最终结果出现“共振”现象，导致结果出错。排查流程就是在git上面做二分查找，最后找到出错的提交记录。\nbug的最小示例\n下面是用于说明 bug 的最小示例。\n#include &lt;bits/stdc++.h&gt;#include &lt;omp.h&gt;using namespace std;class Rng &#123;  static thread_local int seed;public:  static void init(int tid)&#123;    seed = tid * 1000 + 12345; // Example initialization  &#125;  static int get_seed()  &#123;    return seed;  &#125;&#125;;thread_local int Rng::seed = 0; // Initialize the thread-local variableint main()&#123;    int threads = 4;    omp_set_num_threads(threads);    // 第一个并行块    #pragma omp parallel num_threads(threads)    &#123;      #pragma omp critical      cout &lt;&lt; &quot;Thread &quot; &lt;&lt; omp_get_thread_num()            &lt;&lt; &quot; no initialized with seed: &quot;           &lt;&lt; Rng::get_seed() &lt;&lt; endl;      Rng::init(omp_get_thread_num());      #pragma omp critical      cout &lt;&lt; &quot;Thread &quot; &lt;&lt; omp_get_thread_num()            &lt;&lt; &quot; initialized with seed: &quot;           &lt;&lt; Rng::get_seed() &lt;&lt; endl;    &#125;    // 第二个并行块    #pragma omp parallel num_threads(threads)    &#123;      #pragma omp critical      cout &lt;&lt; &quot;Thread &quot; &lt;&lt; omp_get_thread_num()            &lt;&lt; &quot; has seed: &quot;           &lt;&lt; Rng::get_seed() &lt;&lt; endl;    &#125;    // 第三个并行块：带有嵌套并行块的并行块    #pragma omp parallel num_threads(1)    &#123;      #pragma omp parallel num_threads(threads)      &#123;        #pragma omp critical        cout &lt;&lt; &quot;Thread &quot; &lt;&lt; omp_get_thread_num()              &lt;&lt; &quot; in nested parallel region has seed: &quot;             &lt;&lt; Rng::get_seed() &lt;&lt; endl;      &#125;    &#125;&#125;\n使用的是g++实现的openmp。现在请推测：最后有几条输出seed的结果是0？\n\n编译器：g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\ng++ test.cpp -fopenmp -o test\n\n[控制台输出] \n              \n              其中一次的输出结果：Thread 0 no initialized with seed: 0Thread 0 initialized with seed: 12345Thread 2 no initialized with seed: 0Thread 2 initialized with seed: 14345Thread 3 no initialized with seed: 0Thread 3 initialized with seed: 15345Thread 1 no initialized with seed: 0Thread 1 initialized with seed: 13345Thread 0 has seed: 12345Thread 3 has seed: 15345Thread 2 has seed: 14345Thread 1 has seed: 13345Thread 0 in nested parallel region has seed: 12345Thread 1 in nested parallel region has seed: 0Thread 2 in nested parallel region has seed: 0Thread 3 in nested parallel region has seed: 0答案是七个：包括最开始的 4 个，以及嵌套并行块的 3 个。\n              \n            \n下面首先介绍相关的知识点，为最后的问题分析做铺垫。如果想先看分析，可以跳转到这部分。如果看不懂，再看下面两节。\nthread_local\n在c++11标准里面引入了thread_local的关键字。其生命周期和它的宿主线程的生命周期相绑定。一个需要额外注意的点是，thread_local限定的变量，只会初始化一次，也就是说，对于线程而言，其更像是一个静态变量。好，那么对于static thread_local定义的变量呢？其生命周期是怎样的？我们先来看一下cppreference的这一段话：\n\nStorage duration\n\n\nEvery object has a property called storage duration, which limits the object lifetime. There are four kinds of storage duration in C:\n\n\nautomatic storage duration. The storage is allocated when the block in which the object was declared is entered and deallocated when it is exited by any means (goto, return, reaching the end). One exception is the VLAs; their storage is allocated when the declaration is executed, not on block entry, and deallocated when the declaration goes out of scope, not when the block is exited(since C99). If the block is entered recursively, a new allocation is performed for every recursion level. All function parameters and non-static block-scope objects have this storage duration, as well as compound literals used at block scope(until C23)\n\n\nstatic storage duration. The storage duration is the entire execution of the program, and the value stored in the object is initialized only once, prior to main function. **All objects declared static and all objects with either internal or external linkage that aren't declared _Thread_local(until C23)thread_local(since C23)(since C11) have this storage duration.**\n\n\nthread storage duration. The storage duration is the entire execution of the thread in which it was created, and the value stored in the object is initialized when the thread is started. Each thread has its own, distinct, object. If the thread that executes the expression that accesses this object is not the thread that executed its initialization, the behavior is implementation-defined. All objects declared _Thread_local(until C23)thread_local(since C23) have this storage duration.(since C11)\n\n\nallocated storage duration. The storage is allocated and deallocated on request, using dynamic memory allocation functions.\n\n可以推知：static thread_local在生命周期的管理上等同于thread_local。如果还是不信，stackoverflow上面也有人问过同样的问题。\nopenmp的嵌套并行\n众所周知，openmp的线程并行模型是fork-join模型。\nTODO：\n介绍fork-join，介绍syscall，介绍openmp的线程重用机制。\n[小练习] \n              \n              使用g++ -fopenmp编译这段程序，输出结果是什么？可以尝试自行编译检查结果。#include &lt;bits/stdc++.h&gt;#include &lt;omp.h&gt;using namespace std;void foo()&#123;  thread_local int x = 0;  x++;  #pragma omp critical  cout &lt;&lt; &quot;Thread &quot; &lt;&lt; omp_get_thread_num()        &lt;&lt; &quot; has x = &quot; &lt;&lt; x &lt;&lt; endl;&#125;int main()&#123;  #pragma omp parallel num_threads(1)  &#123;    foo();  &#125;  #pragma omp parallel num_threads(1)  &#123;    foo();  &#125;&#125;\n              \n            \n为什么会得到这样的输出\n如果我需要嵌套并行也能实现线程重用，怎么办？\n其他\n关于生命周期\n关于嵌套并行\n其实关于嵌套并行的问题相对来说还是很棘手，因为很多第三方库都可能使用各种各样的线程并行库。如果在你的并行块里面需要调用这些库，那么就会出现套娃的情况，很有可能会创建特别多的线程数，从而影响性能。事实上这个问题一直有人研究，这里提供一篇文献：\n- BOLT: Optimizing OpenMP Parallel Regions with User-Level Threads\n- 对应的 slide\n\n总结\n1.项目管理：这次是 git 立了大功，能够在历史记录上进行二分查找。虽然但是，应该有更好的做法：比如在提交前进行正确性测试，测试正确再进行提交。并且测试应该是可以实现自动化的，这个需要回头了解。\n2.对于静态变量的生命周期，需要仔细考虑；尤其是碰上static thread_local这种组合的时候，需要更加谨慎的处理。\n3.对于g++实现的openmp，其在外层并行块实现了线程重用，但是在内层并行块并没有实现线程重用。更加需要注意的是，因为内层并行块没有实现线程重用，因此内层的线程创建和销毁都有一定开销，在编写高性能计算程序的时候尤其要注意。测试结果是，这个开销大概在1~10ms这个量级。如果需要内层并行块的线程重用，可以使用intel的openmp，应该在intel的oneAPI套件里面，本人并没有调查。编译选项是-kopenmp.\n","categories":["bug沉思录"],"tags":["cpp","bug"]}]